{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGradient descent\n==================\n\nAn example demoing gradient descent by creating figures that trace the\nevolution of the optimizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import optimize\n\nimport sys, os\nsys.path.append(os.path.abspath('helper'))\nfrom cost_functions import mk_quad, mk_gauss, rosenbrock,\\\n    rosenbrock_prime, rosenbrock_hessian, LoggingFunction,\\\n    CountingFunction\n\nx_min, x_max = -1, 2\ny_min, y_max = 2.25/3*x_min - .2, 2.25/3*x_max - .2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A formatter to print values on contours\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def super_fmt(value):\n    if value > 1:\n        if np.abs(int(value) - value) < .1:\n            out = '$10^{%.1i}$' % value\n        else:\n            out = '$10^{%.1f}$' % value\n    else:\n        value = np.exp(value - .01)\n        if value > .1:\n            out = '%1.1f' % value\n        elif value > .01:\n            out = '%.2f' % value\n        else:\n            out = '%.2e' % value\n    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A gradient descent algorithm\ndo not use: its a toy, use scipy's optimize.fmin_cg\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x0, f, f_prime, hessian=None, adaptative=False):\n    x_i, y_i = x0\n    all_x_i = list()\n    all_y_i = list()\n    all_f_i = list()\n\n    for i in range(1, 100):\n        all_x_i.append(x_i)\n        all_y_i.append(y_i)\n        all_f_i.append(f([x_i, y_i]))\n        dx_i, dy_i = f_prime(np.asarray([x_i, y_i]))\n        if adaptative:\n            # Compute a step size using a line_search to satisfy the Wolf\n            # conditions\n            step = optimize.line_search(f, f_prime,\n                                np.r_[x_i, y_i], -np.r_[dx_i, dy_i],\n                                np.r_[dx_i, dy_i], c2=.05)\n            step = step[0]\n            if step is None:\n                step = 0\n        else:\n            step = 1\n        x_i += - step*dx_i\n        y_i += - step*dy_i\n        if np.abs(all_f_i[-1]) < 1e-16:\n            break\n    return all_x_i, all_y_i, all_f_i\n\n\ndef gradient_descent_adaptative(x0, f, f_prime, hessian=None):\n    return gradient_descent(x0, f, f_prime, adaptative=True)\n\n\ndef conjugate_gradient(x0, f, f_prime, hessian=None):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n    optimize.minimize(f, x0, jac=f_prime, method=\"CG\", callback=store, options={\"gtol\": 1e-12})\n    return all_x_i, all_y_i, all_f_i\n\n\ndef newton_cg(x0, f, f_prime, hessian):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n    optimize.minimize(f, x0, method=\"Newton-CG\", jac=f_prime, hess=hessian, callback=store, options={\"xtol\": 1e-12})\n    return all_x_i, all_y_i, all_f_i\n\n\ndef bfgs(x0, f, f_prime, hessian=None):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n    optimize.minimize(f, x0, method=\"BFGS\", jac=f_prime, callback=store, options={\"gtol\": 1e-12})\n    return all_x_i, all_y_i, all_f_i\n\n\ndef powell(x0, f, f_prime, hessian=None):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n    optimize.minimize(f, x0, method=\"Powell\", callback=store, options={\"ftol\": 1e-12})\n    return all_x_i, all_y_i, all_f_i\n\n\ndef nelder_mead(x0, f, f_prime, hessian=None):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n    optimize.minimize(f, x0, method=\"Nelder-Mead\", callback=store, options={\"ftol\": 1e-12})\n    return all_x_i, all_y_i, all_f_i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run different optimizers on these problems\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "levels = dict()\n\nfor index, ((f, f_prime, hessian), optimizer) in enumerate((\n                (mk_quad(.7), gradient_descent),\n                (mk_quad(.7), gradient_descent_adaptative),\n                (mk_quad(.02), gradient_descent),\n                (mk_quad(.02), gradient_descent_adaptative),\n                (mk_gauss(.02), gradient_descent_adaptative),\n                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n                                    gradient_descent_adaptative),\n                (mk_gauss(.02), conjugate_gradient),\n                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n                                    conjugate_gradient),\n                (mk_quad(.02), newton_cg),\n                (mk_gauss(.02), newton_cg),\n                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n                                    newton_cg),\n                (mk_quad(.02), bfgs),\n                (mk_gauss(.02), bfgs),\n                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n                            bfgs),\n                (mk_quad(.02), powell),\n                (mk_gauss(.02), powell),\n                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n                            powell),\n                (mk_gauss(.02), nelder_mead),\n                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n                            nelder_mead),\n            )):\n\n    # Compute a gradient-descent\n    x_i, y_i = 1.6, 1.1\n    counting_f_prime = CountingFunction(f_prime)\n    counting_hessian = CountingFunction(hessian)\n    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n    all_x_i, all_y_i, all_f_i = optimizer(np.array([x_i, y_i]),\n                                          logging_f, counting_f_prime,\n                                          hessian=counting_hessian)\n\n    # Plot the contour plot\n    if not max(all_y_i) < y_max:\n        x_min *= 1.2\n        x_max *= 1.2\n        y_min *= 1.2\n        y_max *= 1.2\n    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n    x = x.T\n    y = y.T\n\n    plt.figure(index, figsize=(3, 2.5))\n    plt.clf()\n    plt.axes([0, 0, 1, 1])\n\n    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n    z = np.apply_along_axis(f, 0, X)\n    log_z = np.log(z + .01)\n    plt.imshow(log_z,\n            extent=[x_min, x_max, y_min, y_max],\n            cmap=plt.cm.gray_r, origin='lower',\n            vmax=log_z.min() + 1.5*log_z.ptp())\n    contours = plt.contour(log_z,\n                        levels=levels.get(f, None),\n                        extent=[x_min, x_max, y_min, y_max],\n                        cmap=plt.cm.gnuplot, origin='lower')\n    levels[f] = contours.levels\n    plt.clabel(contours, inline=1,\n                fmt=super_fmt, fontsize=14)\n\n    plt.plot(all_x_i, all_y_i, 'b-', linewidth=2)\n    plt.plot(all_x_i, all_y_i, 'k+')\n\n    plt.plot(logging_f.all_x_i, logging_f.all_y_i, 'k.', markersize=2)\n\n    plt.plot([0], [0], 'rx', markersize=12)\n\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    plt.draw()\n\n    plt.figure(index + 100, figsize=(4, 3))\n    plt.clf()\n    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2,\n                label='# iterations')\n    plt.ylabel('Error on f(x)')\n    plt.semilogy(logging_f.counts,\n                np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n                linewidth=2, color='g', label='# function calls')\n    plt.legend(loc='upper right', frameon=True, prop=dict(size=11),\n              borderaxespad=0, handlelength=1.5, handletextpad=.5)\n    plt.tight_layout()\n    plt.draw()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}