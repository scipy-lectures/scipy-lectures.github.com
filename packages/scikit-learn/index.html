
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>3.6. scikit-learn: machine learning in Python &#8212; Scipy lecture notes</title>
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/copybutton.js"></script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="3.6.10.1. Measuring Decision Tree performance" href="auto_examples/plot_measuring_performance.html" />
    <link rel="prev" title="3.5. 3D plotting with Mayavi" href="../3d_plotting/index.html" />
   
    <link rel="stylesheet"
	  href="https://unpkg.com/purecss@1.0.0/build/base-min.css">

<script type="text/javascript">
$(function () {
    // Highlight the table of content as we scroll
    sections = {},
    i        = 0,
    url	 = document.URL.replace(/#.*$/, ""),
    current_section = 0;

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50;
    });

    $(window).scroll(function(event) {
	var pos   = $(window).scrollTop();

	// Highlight the current section
	$('a.internal').parent().removeClass('active');
        for(i in sections){
            if(sections[i] > pos){
		break;
            };
	    if($('a.internal[href$="' + i + '"]').is(':visible')){
		current_section = i;
	    };
        }
	$('a.internal[href$="' + current_section + '"]').parent().addClass('active');
	$('a.internal[href$="' + current_section + '"]').parent().parent().parent().addClass('active');
	$('a.internal[href$="' + current_section + '"]').parent().parent().parent().parent().parent().addClass('active');
    });

});
</script>


  </head><body>
   <!-- Use the header to add javascript -->
    

    <script type="text/javascript">
    // Function to collapse the tip divs
    function collapse_tip_div(obj){
	// Update the representation on the tip div based on whether it
	// has the 'collapsed' css class or not: we only want to
	// collapse divs that are not already collapsed
	if($(obj).hasClass("collapsed")) {
	} else {
	    $(obj).find("p.summary").remove();
	    var content = $(obj).text();
	    var html = $(obj).html();

	    if(content.length > 40) {
		if ($.browser.msie) {
		    // We start at '3' to avoid 'tip', as IE
		    // does not count whitespace
		    var content = content.substr(3, 50);
		} else {
		    // We start at '5' to avoid 'tip '
		    var content = content.substr(5, 50);
		}
	    }
	    $(obj).html('<p class="summary"><img src="../../_static/plus.png">' + content + '...</p>' + html);
	}
    }
    </script>

    <script type="text/javascript">
    $(function () {
	$(".tip")
	    .click(function(event){
		$(this).toggleClass("collapsed");
		// Change state of the global button
		$('div.related li.transparent').removeClass('transparent')
		$(this).find("p.summary").remove();
		if($(this).hasClass("collapsed")) {
		    var content = $(this).text();
		    var html = $(this).html();

		    if(content.length > 40) {
			if ($.browser.msie) {
			    // We start at '3' to avoid 'tip', as IE
			    // does not count whitespace
			    var content = content.substr(3, 50);
			} else {
			    // We start at '5' to avoid 'tip '
			    var content = content.substr(5, 50);
			}
		    }
		    $(this).html('<p class="summary"><img src="../../_static/plus.png">' + content + '...</p>' + html);
		}
		if (event.target.tagName.toLowerCase() != "a") {
                   return true; //Makes links clickable
		}
	});
    });
    </script>


    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="auto_examples/plot_measuring_performance.html" title="3.6.10.1. Measuring Decision Tree performance"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="../3d_plotting/index.html" title="3.5. 3D plotting with Mayavi"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">Scipy lecture notes</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U">3. Packages and applications</a> &#187;</li>
     
    <!-- Insert a menu in the navigation bar -->
    <li class="left">
	<!-- On click toggle the 'tip' on or off-->
	<a onclick="$('.tip').each(function (index, obj) {
			    collapse_tip_div(obj);
		    });
		    $('.tip').addClass('collapsed');
		    $('.left').addClass('transparent');">
	<img src="../../_static/minus.png"
         alt="Collapse to compact view" style="padding: 1ex;"/>
	<span class="hiddenlink">Collapse document to compact view</span>
    </a></li>
    <li class="right edit_on_github"><a href="https://github.com/scipy-lectures/scipy-lecture-notes/edit/master/packages/scikit-learn/index.rst">Edit
    <span class="tooltip">
	Improve this page:<br/>Edit it on Github.
    </span>
    </a>
    </li>

      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="scikit-learn-machine-learning-in-python">
<span id="scikit-learn-chapter"></span><h1>3.6. scikit-learn: machine learning in Python<a class="headerlink" href="#scikit-learn-machine-learning-in-python" title="Permalink to this headline">¶</a></h1>
<p><strong>Authors</strong>: <em>Gael Varoquaux</em></p>
<a class="reference internal image-reference" href="../../_images/scikit-learn-logo.png"><img alt="../../_images/scikit-learn-logo.png" class="align-right" src="../../_images/scikit-learn-logo.png" style="width: 226.4px; height: 80.80000000000001px;" /></a>
<div class="topic">
<p class="topic-title">Prerequisites</p>
<ul class="horizontal simple">
<li><a class="reference internal" href="../../intro/numpy/index.html#numpy"><span class="std std-ref">numpy</span></a></li>
<li><a class="reference internal" href="../../intro/scipy.html#scipy"><span class="std std-ref">scipy</span></a></li>
<li><a class="reference internal" href="../../intro/matplotlib/index.html#matplotlib"><span class="std std-ref">matplotlib (optional)</span></a></li>
<li><a class="reference internal" href="../../intro/intro.html#interactive-work"><span class="std std-ref">ipython (the enhancements come handy)</span></a></li>
</ul>
</div>
<div class="sidebar">
<p class="first sidebar-title"><strong>Acknowledgements</strong></p>
<p class="last">This chapter is adapted from <a class="reference external" href="https://www.youtube.com/watch?v=r4bRUvvlaBw">a tutorial</a> given by Gaël
Varoquaux, Jake Vanderplas, Olivier Grisel.</p>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p><strong>Data science in Python</strong></p>
<ul class="last simple">
<li>The <a class="reference internal" href="../statistics/index.html#statistics"><span class="std std-ref">Statistics in Python</span></a> chapter may also be of interest
for readers looking into machine learning.</li>
<li>The <a class="reference external" href="http://scikit-learn.org">documentation of scikit-learn</a> is
very complete and didactic.</li>
</ul>
</div>
<div class="contents local topic" id="chapters-contents">
<p class="topic-title">Chapters contents</p>
<ul class="simple">
<li><a class="reference internal" href="#introduction-problem-settings" id="id25">Introduction: problem settings</a></li>
<li><a class="reference internal" href="#basic-principles-of-machine-learning-with-scikit-learn" id="id26">Basic principles of machine learning with scikit-learn</a></li>
<li><a class="reference internal" href="#supervised-learning-classification-of-handwritten-digits" id="id27">Supervised Learning: Classification of Handwritten Digits</a></li>
<li><a class="reference internal" href="#supervised-learning-regression-of-housing-data" id="id28">Supervised Learning: Regression of Housing Data</a></li>
<li><a class="reference internal" href="#measuring-prediction-performance" id="id29">Measuring prediction performance</a></li>
<li><a class="reference internal" href="#unsupervised-learning-dimensionality-reduction-and-visualization" id="id30">Unsupervised Learning: Dimensionality Reduction and Visualization</a></li>
<li><a class="reference internal" href="#the-eigenfaces-example-chaining-pca-and-svms" id="id31">The eigenfaces example: chaining PCA and SVMs</a></li>
<li><a class="reference internal" href="#id1" id="id32">The eigenfaces example: chaining PCA and SVMs</a></li>
<li><a class="reference internal" href="#parameter-selection-validation-and-testing" id="id33">Parameter selection, Validation, and Testing</a></li>
<li><a class="reference internal" href="#examples-for-the-scikit-learn-chapter" id="id34">Examples for the scikit-learn chapter</a></li>
</ul>
</div>
<div class="section" id="introduction-problem-settings">
<h2><a class="toc-backref" href="#id25">3.6.1. Introduction: problem settings</a><a class="headerlink" href="#introduction-problem-settings" title="Permalink to this headline">¶</a></h2>
<div class="section" id="what-is-machine-learning">
<h3>3.6.1.1. What is machine learning?<a class="headerlink" href="#what-is-machine-learning" title="Permalink to this headline">¶</a></h3>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p>Machine Learning is about building programs with <strong>tunable
parameters</strong> that are adjusted automatically so as to improve their
behavior by <strong>adapting to previously seen data.</strong></p>
<p class="last">Machine Learning can be considered a subfield of <strong>Artificial
Intelligence</strong> since those algorithms can be seen as building blocks
to make computers learn to behave more intelligently by somehow
<strong>generalizing</strong> rather that just storing and retrieving data items
like a database system would do.</p>
</div>
<div class="figure align-right" id="id2">
<a class="reference external image-reference" href="auto_examples/plot_separator.html"><img alt="../../_images/sphx_glr_plot_separator_001.png" src="../../_images/sphx_glr_plot_separator_001.png" style="width: 350px;" /></a>
<p class="caption"><span class="caption-text">A classification problem</span></p>
</div>
<p>We’ll take a look at two very simple machine learning tasks here. The
first is a <strong>classification</strong> task: the figure shows a collection of
two-dimensional data, colored according to two different class labels. A
classification algorithm may be used to draw a dividing boundary between
the two clusters of points:</p>
<p>By drawing this separating line, we have learned a model which can
<strong>generalize</strong> to new data: if you were to drop another point onto the
plane which is unlabeled, this algorithm could now <strong>predict</strong> whether
it’s a blue or a red point.</p>
<div style="flush: both;"></div><div class="figure align-right" id="id3">
<a class="reference external image-reference" href="auto_examples/plot_linear_regression.html"><img alt="../../_images/sphx_glr_plot_linear_regression_001.png" src="../../_images/sphx_glr_plot_linear_regression_001.png" style="width: 350px;" /></a>
<p class="caption"><span class="caption-text">A regression problem</span></p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The next simple task we’ll look at is a <strong>regression</strong> task: a simple
best-fit line to a set of data.</p>
<p>Again, this is an example of fitting a model to data, but our focus here
is that the model can make generalizations about new data. The model has
been <strong>learned</strong> from the training data, and can be used to predict the
result of test data: here, we might be given an x-value, and the model
would allow us to predict the y value.</p>
</div>
<div class="section" id="data-in-scikit-learn">
<h3>3.6.1.2. Data in scikit-learn<a class="headerlink" href="#data-in-scikit-learn" title="Permalink to this headline">¶</a></h3>
<div class="section" id="the-data-matrix">
<h4>The data matrix<a class="headerlink" href="#the-data-matrix" title="Permalink to this headline">¶</a></h4>
<p>Machine learning algorithms implemented in scikit-learn expect data
to be stored in a <strong>two-dimensional array or matrix</strong>. The arrays can be
either <code class="docutils literal notranslate"><span class="pre">numpy</span></code> arrays, or in some cases <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> matrices. The
size of the array is expected to be <code class="docutils literal notranslate"><span class="pre">[n_samples,</span> <span class="pre">n_features]</span></code></p>
<ul class="simple">
<li><strong>n_samples:</strong> The number of samples: each sample is an item to
process (e.g. classify). A sample can be a document, a picture, a
sound, a video, an astronomical object, a row in database or CSV
file, or whatever you can describe with a fixed set of quantitative
traits.</li>
<li><strong>n_features:</strong> The number of features or distinct traits that can
be used to describe each item in a quantitative manner. Features are
generally real-valued, but may be boolean or discrete-valued in some
cases.</li>
</ul>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">The number of features must be fixed in advance. However it can be
very high dimensional (e.g. millions of features) with most of them
being zeros for a given sample. This is a case where <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code>
matrices can be useful, in that they are much more memory-efficient
than numpy arrays.</p>
</div>
</div>
<div class="section" id="a-simple-example-the-iris-dataset">
<h4>A Simple Example: the Iris Dataset<a class="headerlink" href="#a-simple-example-the-iris-dataset" title="Permalink to this headline">¶</a></h4>
<div class="section" id="the-application-problem">
<h5>The application problem<a class="headerlink" href="#the-application-problem" title="Permalink to this headline">¶</a></h5>
<p>As an example of a simple dataset, let us a look at the
iris data stored by scikit-learn. Suppose we want to recognize species of
irises. The data consists of measurements of
three different species of irises:</p>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><img alt="setosa_picture" src="../../_images/iris_setosa.jpg" /></th>
<th class="head"><img alt="versicolor_picture" src="../../_images/iris_versicolor.jpg" /></th>
<th class="head"><img alt="virginica_picture" src="../../_images/iris_virginica.jpg" /></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Setosa Iris</td>
<td>Versicolor Iris</td>
<td>Virginica Iris</td>
</tr>
</tbody>
</table>
<div class="green topic">
<p class="topic-title"><strong>Quick Question:</strong></p>
<blockquote>
<div><p><strong>If we want to design an algorithm to recognize iris species, what
might the data be?</strong></p>
<p>Remember: we need a 2D array of size <code class="docutils literal notranslate"><span class="pre">[n_samples</span> <span class="pre">x</span> <span class="pre">n_features]</span></code>.</p>
<ul class="simple">
<li>What would the <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> refer to?</li>
<li>What might the <code class="docutils literal notranslate"><span class="pre">n_features</span></code> refer to?</li>
</ul>
</div></blockquote>
</div>
<p>Remember that there must be a <strong>fixed</strong> number of features for each
sample, and feature number <code class="docutils literal notranslate"><span class="pre">i</span></code> must be a similar kind of quantity for
each sample.</p>
</div>
<div class="section" id="loading-the-iris-data-with-scikit-learn">
<h5>Loading the Iris Data with Scikit-learn<a class="headerlink" href="#loading-the-iris-data-with-scikit-learn" title="Permalink to this headline">¶</a></h5>
<p>Scikit-learn has a very straightforward set of data on these iris
species. The data consist of the following:</p>
<ul class="simple">
<li>Features in the Iris dataset:<ul class="horizontal">
<li>sepal length (cm)</li>
<li>sepal width (cm)</li>
<li>petal length (cm)</li>
<li>petal width (cm)</li>
</ul>
</li>
<li>Target classes to predict:<ul class="horizontal">
<li>Setosa</li>
<li>Versicolour</li>
<li>Virginica</li>
</ul>
</li>
</ul>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">scikit-learn</span></code> embeds a copy of the iris CSV file along with a
function to load it into numpy arrays:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<div class="newline"></div></pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Import sklearn</strong> Note that scikit-learn is imported as <code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn</span></code></p>
</div>
<p>The features of each sample flower are stored in the <code class="docutils literal notranslate"><span class="pre">data</span></code> attribute
of the dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div><span class="go">(150, 4)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<div class="newline"></div><span class="go">150</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
<div class="newline"></div><span class="go">4</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<div class="newline"></div><span class="go">[5.1  3.5  1.4  0.2]</span>
<div class="newline"></div></pre></div>
</div>
<p>The information about the class of each sample is stored in the
<code class="docutils literal notranslate"><span class="pre">target</span></code> attribute of the dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div><span class="go">(150,)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<div class="newline"></div><span class="go">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span>
<div class="newline"></div><span class="go"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span>
<div class="newline"></div><span class="go"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2</span>
<div class="newline"></div><span class="go"> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span>
<div class="newline"></div><span class="go"> 2 2]</span>
<div class="newline"></div></pre></div>
</div>
<p>The names of the classes are stored in the last attribute, namely
<code class="docutils literal notranslate"><span class="pre">target_names</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<div class="newline"></div><span class="go">[&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;]</span>
<div class="newline"></div></pre></div>
</div>
<p>This data is four-dimensional, but we can visualize two of the
dimensions at a time using a scatter plot:</p>
<a class="reference external image-reference" href="auto_examples/plot_iris_scatter.html"><img alt="../../_images/sphx_glr_plot_iris_scatter_001.png" class="align-left" src="../../_images/sphx_glr_plot_iris_scatter_001.png" /></a>
<div class="green topic">
<p class="topic-title"><strong>Exercise</strong>:</p>
<p>Can you choose 2 features to find a plot where it is easier to
seperate the different classes of irises?</p>
<p><strong>Hint</strong>: click on the figure above to see the code that generates it,
and modify this code.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="basic-principles-of-machine-learning-with-scikit-learn">
<h2><a class="toc-backref" href="#id26">3.6.2. Basic principles of machine learning with scikit-learn</a><a class="headerlink" href="#basic-principles-of-machine-learning-with-scikit-learn" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introducing-the-scikit-learn-estimator-object">
<h3>3.6.2.1. Introducing the scikit-learn estimator object<a class="headerlink" href="#introducing-the-scikit-learn-estimator-object" title="Permalink to this headline">¶</a></h3>
<p>Every algorithm is exposed in scikit-learn via an ‘’Estimator’’ object.
For instance a linear regression is: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.LinearRegression</span></code></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<div class="newline"></div></pre></div>
</div>
<p><strong>Estimator parameters</strong>: All the parameters of an estimator can be set
when it is instantiated:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">normalize</span><span class="p">)</span>
<div class="newline"></div><span class="go">True</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<div class="newline"></div><span class="go">LinearRegression(n_jobs=1, normalize=True)</span>
<div class="newline"></div></pre></div>
</div>
<div class="section" id="fitting-on-data">
<h4>Fitting on data<a class="headerlink" href="#fitting-on-data" title="Permalink to this headline">¶</a></h4>
<p>Let’s create some simple data with <a class="reference internal" href="../../intro/numpy/index.html#numpy"><span class="std std-ref">numpy</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="c1"># The input data for sklearn is 2D: (samples == 3 x features == 1)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<div class="newline"></div><span class="go">array([[0],</span>
<div class="newline"></div><span class="go">       [1],</span>
<div class="newline"></div><span class="go">       [2]])</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div><span class="go">LinearRegression(n_jobs=1, normalize=True)</span>
<div class="newline"></div></pre></div>
</div>
<p><strong>Estimated parameters</strong>: When data is fitted with an estimator,
parameters are estimated from the data at hand. All the estimated
parameters are attributes of the estimator object ending by an
underscore:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
<div class="newline"></div><span class="go">array([1.])</span>
<div class="newline"></div></pre></div>
</div>
</div>
</div>
<div class="section" id="supervised-learning-classification-and-regression">
<h3>3.6.2.2. Supervised Learning: Classification and regression<a class="headerlink" href="#supervised-learning-classification-and-regression" title="Permalink to this headline">¶</a></h3>
<p>In <strong>Supervised Learning</strong>, we have a dataset consisting of both
features and labels. The task is to construct an estimator which is able
to predict the label of an object given the set of features. A
relatively simple example is predicting the species of iris given a set
of measurements of its flower. This is a relatively simple task. Some
more complicated examples are:</p>
<ul class="simple">
<li>given a multicolor image of an object through a telescope, determine
whether that object is a star, a quasar, or a galaxy.</li>
<li>given a photograph of a person, identify the person in the photo.</li>
<li>given a list of movies a person has watched and their personal rating
of the movie, recommend a list of movies they would like (So-called
<em>recommender systems</em>: a famous example is the <a class="reference external" href="http://en.wikipedia.org/wiki/Netflix_prize">Netflix
Prize</a>).</li>
</ul>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">What these tasks have in common is that there is one or more unknown
quantities associated with the object which needs to be determined from
other observed quantities.</p>
</div>
<p>Supervised learning is further broken down into two categories,
<strong>classification</strong> and <strong>regression</strong>. In classification, the label is
discrete, while in regression, the label is continuous. For example, in
astronomy, the task of determining whether an object is a star, a
galaxy, or a quasar is a classification problem: the label is from three
distinct categories. On the other hand, we might wish to estimate the
age of an object based on such observations: this would be a regression
problem, because the label (age) is a continuous quantity.</p>
<p><strong>Classification</strong>: K nearest neighbors (kNN) is one of the simplest
learning strategies: given a new, unknown observation, look up in your
reference database which ones have the closest features and assign the
predominant class. Let’s try it out on our iris classification problem:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">datasets</span>
<div class="newline"></div><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<div class="newline"></div><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<div class="newline"></div><span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<div class="newline"></div><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div><span class="c1"># What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?</span>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])])</span>
<div class="newline"></div></pre></div>
</div>
<div class="figure align-center" id="id4">
<a class="reference external image-reference" href="auto_examples/plot_iris_knn.html"><img alt="../../_images/sphx_glr_plot_iris_knn_001.png" src="../../_images/sphx_glr_plot_iris_knn_001.png" /></a>
<p class="caption"><span class="caption-text">A plot of the sepal space and the prediction of the KNN</span></p>
</div>
<p><strong>Regression</strong>: The simplest possible regression setting is the linear
regression one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<div class="newline"></div>
<div class="newline"></div><span class="c1"># x from 0 to 30</span>
<div class="newline"></div><span class="n">x</span> <span class="o">=</span> <span class="mi">30</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<div class="newline"></div>
<div class="newline"></div><span class="c1"># y = a*x + b with noise</span>
<div class="newline"></div><span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="c1"># create a linear regression model</span>
<div class="newline"></div><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<div class="newline"></div><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="c1"># predict y from the data</span>
<div class="newline"></div><span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<div class="newline"></div><span class="n">y_new</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_new</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<div class="newline"></div>
<div class="newline"></div></pre></div>
</div>
<div class="figure align-center" id="id5">
<a class="reference external image-reference" href="auto_examples/plot_linear_regression.html"><img alt="../../_images/sphx_glr_plot_linear_regression_001.png" src="../../_images/sphx_glr_plot_linear_regression_001.png" /></a>
<p class="caption"><span class="caption-text">A plot of a simple linear regression.</span></p>
</div>
</div>
<div class="section" id="a-recap-on-scikit-learn-s-estimator-interface">
<h3>3.6.2.3. A recap on Scikit-learn’s estimator interface<a class="headerlink" href="#a-recap-on-scikit-learn-s-estimator-interface" title="Permalink to this headline">¶</a></h3>
<p>Scikit-learn strives to have a uniform interface across all methods, and
we’ll see examples of these below. Given a scikit-learn <em>estimator</em>
object named <code class="docutils literal notranslate"><span class="pre">model</span></code>, the following methods are available:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">In <strong>all Estimators</strong>:</th></tr>
<tr class="field-odd field"><td>&#160;</td><td class="field-body"><ul class="first simple">
<li><code class="docutils literal notranslate"><span class="pre">model.fit()</span></code> : fit training data. For supervised learning
applications, this accepts two arguments: the data <code class="docutils literal notranslate"><span class="pre">X</span></code> and the
labels <code class="docutils literal notranslate"><span class="pre">y</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">model.fit(X,</span> <span class="pre">y)</span></code>). For unsupervised learning
applications, this accepts only a single argument, the data <code class="docutils literal notranslate"><span class="pre">X</span></code>
(e.g. <code class="docutils literal notranslate"><span class="pre">model.fit(X)</span></code>).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">In <strong>supervised estimators</strong>:</th></tr>
<tr class="field-even field"><td>&#160;</td><td class="field-body"><ul class="first simple">
<li><code class="docutils literal notranslate"><span class="pre">model.predict()</span></code> : given a trained model, predict the label of a
new set of data. This method accepts one argument, the new data
<code class="docutils literal notranslate"><span class="pre">X_new</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">model.predict(X_new)</span></code>), and returns the learned
label for each object in the array.</li>
<li><code class="docutils literal notranslate"><span class="pre">model.predict_proba()</span></code> : For classification problems, some
estimators also provide this method, which returns the probability
that a new observation has each categorical label. In this case, the
label with the highest probability is returned by
<code class="docutils literal notranslate"><span class="pre">model.predict()</span></code>.</li>
<li><code class="docutils literal notranslate"><span class="pre">model.score()</span></code> : for classification or regression problems, most
(all?) estimators implement a score method. Scores are between 0 and
1, with a larger score indicating a better fit.</li>
</ul>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">In <strong>unsupervised estimators</strong>:</th></tr>
<tr class="field-odd field"><td>&#160;</td><td class="field-body"><ul class="first last simple">
<li><code class="docutils literal notranslate"><span class="pre">model.transform()</span></code> : given an unsupervised model, transform new
data into the new basis. This also accepts one argument <code class="docutils literal notranslate"><span class="pre">X_new</span></code>,
and returns the new representation of the data based on the
unsupervised model.</li>
<li><code class="docutils literal notranslate"><span class="pre">model.fit_transform()</span></code> : some estimators implement this method,
which more efficiently performs a fit and a transform on the same
input data.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="regularization-what-it-is-and-why-it-is-necessary">
<h3>3.6.2.4. Regularization: what it is and why it is necessary<a class="headerlink" href="#regularization-what-it-is-and-why-it-is-necessary" title="Permalink to this headline">¶</a></h3>
<div class="section" id="prefering-simpler-models">
<h4>Prefering simpler models<a class="headerlink" href="#prefering-simpler-models" title="Permalink to this headline">¶</a></h4>
<p><strong>Train errors</strong> Suppose you are using a 1-nearest neighbor estimator.
How many errors do you expect on your train set?</p>
<ul class="simple">
<li>Train set error is not a good measurement of prediction performance.
You need to leave out a test set.</li>
<li>In general, we should accept errors on the train set.</li>
</ul>
<p><strong>An example of regularization</strong> The core idea behind regularization is
that we are going to prefer models that are simpler, for a certain
definition of ‘’simpler’’, even if they lead to more errors on the train
set.</p>
<p>As an example, let’s generate with a 9th order polynomial, with noise:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="auto_examples/plot_polynomial_regression.html"><img alt="../../_images/sphx_glr_plot_polynomial_regression_001.png" src="../../_images/sphx_glr_plot_polynomial_regression_001.png" style="width: 540.0px; height: 360.0px;" /></a>
</div>
<p>And now, let’s fit a 4th order and a 9th order polynomial to the data.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="auto_examples/plot_polynomial_regression.html"><img alt="../../_images/sphx_glr_plot_polynomial_regression_002.png" src="../../_images/sphx_glr_plot_polynomial_regression_002.png" style="width: 540.0px; height: 360.0px;" /></a>
</div>
<p>With your naked eyes, which model do you prefer, the 4th order one, or
the 9th order one?</p>
<p>Let’s look at the ground truth:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="auto_examples/plot_polynomial_regression.html"><img alt="../../_images/sphx_glr_plot_polynomial_regression_003.png" src="../../_images/sphx_glr_plot_polynomial_regression_003.png" style="width: 540.0px; height: 360.0px;" /></a>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Regularization is ubiquitous in machine learning. Most scikit-learn
estimators have a parameter to tune the amount of regularization. For
instance, with k-NN, it is ‘k’, the number of nearest neighbors used to
make the decision. k=1 amounts to no regularization: 0 error on the
training set, whereas large k will push toward smoother decision
boundaries in the feature space.</p>
</div>
</div>
<div class="section" id="simple-versus-complex-models-for-classification">
<h4>Simple versus complex models for classification<a class="headerlink" href="#simple-versus-complex-models-for-classification" title="Permalink to this headline">¶</a></h4>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><a class="reference external" href="auto_examples/plot_svm_non_linear.html"><img alt="linear" src="../../_images/sphx_glr_plot_svm_non_linear_001.png" style="width: 400px;" /></a></th>
<th class="head"><a class="reference external" href="auto_examples/plot_svm_non_linear.html"><img alt="nonlinear" src="../../_images/sphx_glr_plot_svm_non_linear_002.png" style="width: 400px;" /></a></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>A linear separation</td>
<td>A non-linear separation</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">For classification models, the decision boundary, that separates the
class expresses the complexity of the model. For instance, a linear
model, that makes a decision based on a linear combination of
features, is more complex than a non-linear one.</p>
</div>
</div>
</div>
</div>
<div class="section" id="supervised-learning-classification-of-handwritten-digits">
<h2><a class="toc-backref" href="#id27">3.6.3. Supervised Learning: Classification of Handwritten Digits</a><a class="headerlink" href="#supervised-learning-classification-of-handwritten-digits" title="Permalink to this headline">¶</a></h2>
<div class="section" id="the-nature-of-the-data">
<h3>3.6.3.1. The nature of the data<a class="headerlink" href="#the-nature-of-the-data" title="Permalink to this headline">¶</a></h3>
<div class="sidebar">
<p class="first sidebar-title">Code and notebook</p>
<p class="last">Python code and Jupyter notebook for this section are found
<a class="reference internal" href="auto_examples/plot_digits_simple_classif.html#sphx-glr-packages-scikit-learn-auto-examples-plot-digits-simple-classif-py"><span class="std std-ref">here</span></a></p>
</div>
<p>In this section we’ll apply scikit-learn to the classification of
handwritten digits. This will go a bit beyond the iris classification we
saw before: we’ll discuss some of the metrics which can be used in
evaluating the effectiveness of a classification model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_digits_simple_classif.html"><img alt="../../_images/sphx_glr_plot_digits_simple_classif_001.png" class="align-center" src="../../_images/sphx_glr_plot_digits_simple_classif_001.png" /></a>
<p>Let us visualize the data and remind us what we’re looking at (click on
the figure for the full code):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the digits: each image is 8x8 pixels</span>
<div class="newline"></div><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
</div>
<div class="section" id="visualizing-the-data-on-its-principal-components">
<h3>3.6.3.2. Visualizing the Data on its principal components<a class="headerlink" href="#visualizing-the-data-on-its-principal-components" title="Permalink to this headline">¶</a></h3>
<p>A good first-step for many problems is to visualize the data using a
<em>Dimensionality Reduction</em> technique. We’ll start with the most
straightforward one, <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>.</p>
<p>PCA seeks orthogonal linear combinations of the features which show the
greatest variance, and as such, can help give you a good idea of the
structure of the data set.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">proj</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">proj</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">proj</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> 
<div class="newline"></div><span class="go">&lt;matplotlib.collections.PathCollection object at ...&gt;</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span> 
<div class="newline"></div><span class="go">&lt;matplotlib.colorbar.Colorbar object at ...&gt;</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_digits_simple_classif.html"><img alt="../../_images/sphx_glr_plot_digits_simple_classif_002.png" class="align-center" src="../../_images/sphx_glr_plot_digits_simple_classif_002.png" /></a>
<div class="green topic">
<p class="topic-title"><strong>Question</strong></p>
<p>Given these projections of the data, which numbers do you think a
classifier might have trouble distinguishing?</p>
</div>
</div>
<div class="section" id="gaussian-naive-bayes-classification">
<h3>3.6.3.3. Gaussian Naive Bayes Classification<a class="headerlink" href="#gaussian-naive-bayes-classification" title="Permalink to this headline">¶</a></h3>
<p>For most classification problems, it’s nice to have a simple, fast
method to provide a quick baseline classification. If the simple
and fast method is sufficient, then we don’t have to waste CPU cycles on
more complex models. If not, we can use the results of the simple method
to give us clues about our data.</p>
<p>One good method to keep in mind is Gaussian Naive Bayes
(<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.naive_bayes.GaussianNB</span></code></a>).</p>
<div class="sidebar">
<p class="first sidebar-title">Old scikit-learn versions</p>
<p class="last"><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split" title="(in scikit-learn v1.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">train_test_split()</span></code></a> is imported from
<code class="docutils literal notranslate"><span class="pre">sklearn.cross_validation</span></code></p>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Gaussian Naive Bayes fits a Gaussian distribution to each training label
independantly on each feature, and uses this to quickly give a rough
classification. It is generally not sufficiently accurate for real-world
data, but can perform surprisingly well, for instance on text data.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># split the data into training and validation sets</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># train the model</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<div class="newline"></div><span class="go">GaussianNB()</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># use the model to predict the labels of the test data</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">predicted</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="n">y_test</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span> 
<div class="newline"></div><span class="go">[1 7 7 7 8 2 8 0 4 8 7 7 0 8 2 3 5 8 5 3 7 9 6 2 8 2 2 7 3 5...]</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">expected</span><span class="p">)</span> 
<div class="newline"></div><span class="go">[1 0 4 7 8 2 2 0 4 3 7 7 0 8 2 3 4 8 5 3 7 9 6 3 8 2 2 9 3 5...]</span>
<div class="newline"></div></pre></div>
</div>
<p>As above, we plot the digits with the predicted labels to get an idea of
how well the classification is working.</p>
<a class="reference external image-reference" href="auto_examples/plot_digits_simple_classif.html"><img alt="../../_images/sphx_glr_plot_digits_simple_classif_003.png" class="align-center" src="../../_images/sphx_glr_plot_digits_simple_classif_003.png" /></a>
<div class="green topic">
<p class="topic-title"><strong>Question</strong></p>
<p>Why did we split the data into training and validation sets?</p>
</div>
</div>
<div class="section" id="quantitative-measurement-of-performance">
<h3>3.6.3.4. Quantitative Measurement of Performance<a class="headerlink" href="#quantitative-measurement-of-performance" title="Permalink to this headline">¶</a></h3>
<p>We’d like to measure the performance of our estimator without having to
resort to plotting examples. A simple method might be to simply compare
the number of matches:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">matches</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">expected</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">matches</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<div class="newline"></div><span class="go">367</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">matches</span><span class="p">))</span>
<div class="newline"></div><span class="go">450</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">matches</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">matches</span><span class="p">))</span>
<div class="newline"></div><span class="go">0.81555555555555559</span>
<div class="newline"></div></pre></div>
</div>
<p>We see that more than 80% of the 450 predictions match the input. But
there are other more sophisticated metrics that can be used to judge the
performance of a classifier: several are available in the
<a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics" title="(in scikit-learn v1.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a> submodule.</p>
<p>One of the most useful metrics is the <code class="docutils literal notranslate"><span class="pre">classification_report</span></code>, which
combines several measures and prints a table with the results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">))</span>
<div class="newline"></div><span class="go">            precision    recall  f1-score   support</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">         0       1.00      0.91      0.95        46</span>
<div class="newline"></div><span class="go">         1       0.76      0.64      0.69        44</span>
<div class="newline"></div><span class="go">         2       0.85      0.62      0.72        47</span>
<div class="newline"></div><span class="go">         3       0.98      0.82      0.89        49</span>
<div class="newline"></div><span class="go">         4       0.89      0.86      0.88        37</span>
<div class="newline"></div><span class="go">         5       0.97      0.93      0.95        41</span>
<div class="newline"></div><span class="go">         6       1.00      0.98      0.99        44</span>
<div class="newline"></div><span class="go">         7       0.73      1.00      0.84        45</span>
<div class="newline"></div><span class="go">         8       0.50      0.90      0.64        49</span>
<div class="newline"></div><span class="go">         9       0.93      0.54      0.68        48</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">    accuracy                           0.82       450</span>
<div class="newline"></div><span class="go">   macro avg       0.86      0.82      0.82       450</span>
<div class="newline"></div><span class="go">weighted avg       0.86      0.82      0.82       450</span>
<div class="newline"></div></pre></div>
</div>
<p>Another enlightening metric for this sort of multi-label classification
is a <em>confusion matrix</em>: it helps us visualize which labels are being
interchanged in the classification errors:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">))</span>
<div class="newline"></div><span class="go">[[42  0  0  0  3  0  0  1  0  0]</span>
<div class="newline"></div><span class="go"> [ 0 28  0  0  0  0  0  1 13  2]</span>
<div class="newline"></div><span class="go"> [ 0  3 29  0  0  0  0  0 15  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  2 40  0  0  0  2  5  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  1  0 32  1  0  3  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  0  0  0 38  0  2  1  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  1  0  0  0 43  0  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  0  0  0  0  0 45  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  3  1  0  0  0  0  1 44  0]</span>
<div class="newline"></div><span class="go"> [ 0  3  0  1  1  0  0  7 10 26]]</span>
<div class="newline"></div></pre></div>
</div>
<p>We see here that in particular, the numbers 1, 2, 3, and 9 are often
being labeled 8.</p>
</div>
</div>
<div class="section" id="supervised-learning-regression-of-housing-data">
<h2><a class="toc-backref" href="#id28">3.6.4. Supervised Learning: Regression of Housing Data</a><a class="headerlink" href="#supervised-learning-regression-of-housing-data" title="Permalink to this headline">¶</a></h2>
<p>Here we’ll do a short example of a regression problem: learning a
continuous value from a set of features.</p>
<div class="section" id="a-quick-look-at-the-data">
<h3>3.6.4.1. A quick look at the data<a class="headerlink" href="#a-quick-look-at-the-data" title="Permalink to this headline">¶</a></h3>
<div class="sidebar">
<p class="first sidebar-title">Code and notebook</p>
<p class="last">Python code and Jupyter notebook for this section are found
<a class="reference internal" href="auto_examples/plot_california_prediction.html#sphx-glr-packages-scikit-learn-auto-examples-plot-california-prediction-py"><span class="std std-ref">here</span></a></p>
</div>
<p>We’ll use the California house prices set, available in scikit-learn.
This records measurements of 8 attributes of housing markets in
California, as well as the median price. The question is: can you predict
the price of a new market given its attributes?:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div><span class="go">(20640, 8)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div><span class="go">(20640,)</span>
<div class="newline"></div></pre></div>
</div>
<p>We can see that there are just over 20000 data points.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">DESCR</span></code> variable has a long description of the dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span> 
<div class="newline"></div><span class="go">.. _california_housing_dataset:</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">California Housing dataset</span>
<div class="newline"></div><span class="go">--------------------------</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">**Data Set Characteristics:**</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">    :Number of Instances: 20640</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">    :Number of Attributes: 8 numeric, predictive attributes and the target</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">    :Attribute Information:</span>
<div class="newline"></div><span class="go">        - MedInc        median income in block</span>
<div class="newline"></div><span class="go">        - HouseAge      median house age in block</span>
<div class="newline"></div><span class="go">        - AveRooms      average number of rooms</span>
<div class="newline"></div><span class="go">        - AveBedrms     average number of bedrooms</span>
<div class="newline"></div><span class="go">        - Population    block population</span>
<div class="newline"></div><span class="go">        - AveOccup      average house occupancy</span>
<div class="newline"></div><span class="go">        - Latitude      house block latitude</span>
<div class="newline"></div><span class="go">        - Longitude     house block longitude</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">    :Missing Attribute Values: None</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">This dataset was obtained from the StatLib repository.</span>
<div class="newline"></div><span class="go">http://lib.stat.cmu.edu/datasets/</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">The target variable is the median house value for California districts.</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">This dataset was derived from the 1990 U.S. census, using one row per census</span>
<div class="newline"></div><span class="go">block group. A block group is the smallest geographical unit for which the U.S.</span>
<div class="newline"></div><span class="go">Census Bureau publishes sample data (a block group typically has a population</span>
<div class="newline"></div><span class="go">of 600 to 3,000 people).</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">It can be downloaded/loaded using the</span>
<div class="newline"></div><span class="go">:func:`sklearn.datasets.fetch_california_housing` function.</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">.. topic:: References</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,</span>
<div class="newline"></div><span class="go">      Statistics and Probability Letters, 33 (1997) 291-297</span>
<div class="newline"></div></pre></div>
</div>
<p>It often helps to quickly visualize pieces of the data using histograms,
scatter plots, or other plot types. With matplotlib, let us show a
histogram of the target values: the median price in each neighborhood:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>  
<div class="newline"></div><span class="go">(array([...</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_001.png" class="align-center" src="../../_images/sphx_glr_plot_california_prediction_001.png" style="width: 280.0px; height: 210.0px;" /></a>
<p>Let’s have a quick look to see if some features are more relevant than
others for our problem:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">feature_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">feature_name</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>  
<div class="newline"></div><span class="go">&lt;Figure size...</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_002.png" src="../../_images/sphx_glr_plot_california_prediction_002.png" style="width: 32%;" /></a>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_003.png" src="../../_images/sphx_glr_plot_california_prediction_003.png" style="width: 32%;" /></a>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_004.png" src="../../_images/sphx_glr_plot_california_prediction_004.png" style="width: 32%;" /></a>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_005.png" src="../../_images/sphx_glr_plot_california_prediction_005.png" style="width: 32%;" /></a>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_006.png" src="../../_images/sphx_glr_plot_california_prediction_006.png" style="width: 32%;" /></a>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_007.png" src="../../_images/sphx_glr_plot_california_prediction_007.png" style="width: 32%;" /></a>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_008.png" src="../../_images/sphx_glr_plot_california_prediction_008.png" style="width: 32%;" /></a>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_009.png" src="../../_images/sphx_glr_plot_california_prediction_009.png" style="width: 32%;" /></a>
<p>This is a manual version of a technique called <strong>feature selection</strong>.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Sometimes, in Machine Learning it is useful to use feature selection to
decide which features are the most useful for a particular problem.
Automated methods exist which quantify this sort of exercise of choosing
the most informative features.</p>
</div>
</div>
<div class="section" id="predicting-home-prices-a-simple-linear-regression">
<h3>3.6.4.2. Predicting Home Prices: a Simple Linear Regression<a class="headerlink" href="#predicting-home-prices-a-simple-linear-regression" title="Permalink to this headline">¶</a></h3>
<p>Now we’ll use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> to perform a simple linear regression on
the housing data. There are many possibilities of regressors to use. A
particularly simple one is <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>: this is basically a
wrapper around an ordinary least squares calculation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<div class="newline"></div><span class="go">LinearRegression()</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">predicted</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="n">y_test</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMS: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">predicted</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)))</span> 
<div class="newline"></div><span class="go">RMS: 0.7...</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_california_prediction.html"><img alt="../../_images/sphx_glr_plot_california_prediction_010.png" class="align-right" src="../../_images/sphx_glr_plot_california_prediction_010.png" /></a>
<p>We can plot the error: expected as a function of predicted:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span> 
<div class="newline"></div><span class="go">&lt;matplotlib.collections.PathCollection object at ...&gt;</span>
<div class="newline"></div></pre></div>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">The prediction at least correlates with the true price, though there are
clearly some biases. We could imagine evaluating the performance of the
regressor by, say, computing the RMS residuals between the true and
predicted price. There are some subtleties in this, however, which we’ll
cover in a later section.</p>
</div>
<div class="green topic">
<p class="topic-title"><strong>Exercise: Gradient Boosting Tree Regression</strong></p>
<p>There are many other types of regressors available in scikit-learn:
we’ll try a more powerful one here.</p>
<p><strong>Use the GradientBoostingRegressor class to fit the housing data</strong>.</p>
<p><strong>hint</strong> You can copy and paste some of the above code, replacing
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> with
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<div class="newline"></div><span class="c1"># Instantiate the model, fit the results, and scatter in vs. out</span>
<div class="newline"></div></pre></div>
</div>
<p><strong>Solution</strong> The solution is found in <a class="reference internal" href="auto_examples/plot_california_prediction.html#sphx-glr-packages-scikit-learn-auto-examples-plot-california-prediction-py"><span class="std std-ref">the code of this chapter</span></a></p>
</div>
</div>
</div>
<div class="section" id="measuring-prediction-performance">
<h2><a class="toc-backref" href="#id29">3.6.5. Measuring prediction performance</a><a class="headerlink" href="#measuring-prediction-performance" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-quick-test-on-the-k-neighbors-classifier">
<h3>3.6.5.1. A quick test on the K-neighbors classifier<a class="headerlink" href="#a-quick-test-on-the-k-neighbors-classifier" title="Permalink to this headline">¶</a></h3>
<p>Here we’ll continue to look at the digits data, but we’ll switch to the
K-Neighbors classifier.  The K-neighbors classifier is an instance-based
classifier.  The K-neighbors classifier predicts the label of
an unknown point based on the labels of the <em>K</em> nearest points in the
parameter space.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Get the data</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Instantiate and train the classifier</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<div class="newline"></div><span class="go">KNeighborsClassifier(...)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Check the results using metrics</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<div class="newline"></div><span class="go">[[178   0   0   0   0   0   0   0   0   0]</span>
<div class="newline"></div><span class="go"> [  0 182   0   0   0   0   0   0   0   0]</span>
<div class="newline"></div><span class="go"> [  0   0 177   0   0   0   0   0   0   0]</span>
<div class="newline"></div><span class="go"> [  0   0   0 183   0   0   0   0   0   0]</span>
<div class="newline"></div><span class="go"> [  0   0   0   0 181   0   0   0   0   0]</span>
<div class="newline"></div><span class="go"> [  0   0   0   0   0 182   0   0   0   0]</span>
<div class="newline"></div><span class="go"> [  0   0   0   0   0   0 181   0   0   0]</span>
<div class="newline"></div><span class="go"> [  0   0   0   0   0   0   0 179   0   0]</span>
<div class="newline"></div><span class="go"> [  0   0   0   0   0   0   0   0 174   0]</span>
<div class="newline"></div><span class="go"> [  0   0   0   0   0   0   0   0   0 180]]</span>
<div class="newline"></div></pre></div>
</div>
<p>Apparently, we’ve found a perfect classifier!  But this is misleading for
the reasons we saw before: the classifier essentially “memorizes” all the
samples it has already seen.  To really test how well this algorithm
does, we need to try some samples it <em>hasn’t</em> yet seen.</p>
<p>This problem also occurs with regression models. In the following we
fit an other instance-based model named “decision tree” to the California
Housing price dataset we introduced previously:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">predicted</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span> 
<div class="newline"></div><span class="go">&lt;matplotlib.collections.PathCollection object at ...&gt;</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="s1">&#39;--k&#39;</span><span class="p">)</span> 
<div class="newline"></div><span class="go">[&lt;matplotlib.lines.Line2D object at ...]</span>
<div class="newline"></div></pre></div>
</div>
<div class="figure align-right">
<a class="reference external image-reference" href="auto_examples/plot_measuring_performance.html"><img alt="../../_images/sphx_glr_plot_measuring_performance_001.png" src="../../_images/sphx_glr_plot_measuring_performance_001.png" style="width: 350px;" /></a>
</div>
<p>Here again the predictions are seemingly perfect as the model was able to
perfectly memorize the training set.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p><strong>Performance on test set</strong></p>
<p class="last">Performance on test set does not measure overfit (as described above)</p>
</div>
</div>
<div class="section" id="a-correct-approach-using-a-validation-set">
<h3>3.6.5.2. A correct approach: Using a validation set<a class="headerlink" href="#a-correct-approach-using-a-validation-set" title="Permalink to this headline">¶</a></h3>
<p>Learning the parameters of a prediction function and testing it on the
same data is a methodological mistake: a model that would just repeat the
labels of the samples that it has just seen would have a perfect score
but would fail to predict anything useful on yet-unseen data.</p>
<p>To avoid over-fitting, we have to define two different sets:</p>
<ul class="simple">
<li>a training set X_train, y_train which is used for learning the
parameters of a predictive model</li>
<li>a testing set X_test, y_test which is used for evaluating the fitted
predictive model</li>
</ul>
<p>In scikit-learn such a random split can be quickly computed with the
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split" title="(in scikit-learn v1.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">train_test_split()</span></code></a> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">model_selection</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
<div class="newline"></div><span class="gp">... </span>                                        <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%r</span><span class="s2">, </span><span class="si">%r</span><span class="s2">, </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<div class="newline"></div><span class="go">(1797, 64), (1347, 64), (450, 64)</span>
<div class="newline"></div></pre></div>
</div>
<p>Now we train on the training data, and test on the testing data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<div class="newline"></div><span class="go">[[37  0  0  0  0  0  0  0  0  0]</span>
<div class="newline"></div><span class="go"> [ 0 43  0  0  0  0  0  0  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  0 43  1  0  0  0  0  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  0 45  0  0  0  0  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  0  0 38  0  0  0  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  0  0  0 47  0  0  0  1]</span>
<div class="newline"></div><span class="go"> [ 0  0  0  0  0  0 52  0  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  0  0  0  0  0 48  0  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  0  0  0  0  0  0 48  0]</span>
<div class="newline"></div><span class="go"> [ 0  0  0  1  0  1  0  0  0 45]]</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<div class="newline"></div><span class="go">              precision    recall  f1-score   support</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">           0       1.00      1.00      1.00        37</span>
<div class="newline"></div><span class="go">           1       1.00      1.00      1.00        43</span>
<div class="newline"></div><span class="go">           2       1.00      0.98      0.99        44</span>
<div class="newline"></div><span class="go">           3       0.96      1.00      0.98        45</span>
<div class="newline"></div><span class="go">           4       1.00      1.00      1.00        38</span>
<div class="newline"></div><span class="go">           5       0.98      0.98      0.98        48</span>
<div class="newline"></div><span class="go">           6       1.00      1.00      1.00        52</span>
<div class="newline"></div><span class="go">           7       1.00      1.00      1.00        48</span>
<div class="newline"></div><span class="go">           8       1.00      1.00      1.00        48</span>
<div class="newline"></div><span class="go">           9       0.98      0.96      0.97        47</span>
<div class="newline"></div>
<div class="newline"></div><span class="go">    accuracy                           0.99       450</span>
<div class="newline"></div><span class="go">   macro avg       0.99      0.99      0.99       450</span>
<div class="newline"></div><span class="go">weighted avg       0.99      0.99      0.99       450</span>
<div class="newline"></div></pre></div>
</div>
<p>The averaged f1-score is often used as a convenient measure of the
overall performance of an algorithm.  It appears in the bottom row
of the classification report; it can also be accessed directly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">)</span> 
<div class="newline"></div><span class="go">0.991367...</span>
<div class="newline"></div></pre></div>
</div>
<p>The over-fitting we saw previously can be quantified by computing the
f1-score on the training data itself:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">)</span>
<div class="newline"></div><span class="go">1.0</span>
<div class="newline"></div></pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Regression metrics</strong> In the case of regression models, we
need to use different metrics, such as explained variance.</p>
</div>
</div>
<div class="section" id="model-selection-via-validation">
<h3>3.6.5.3. Model Selection via Validation<a class="headerlink" href="#model-selection-via-validation" title="Permalink to this headline">¶</a></h3>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">We have applied Gaussian Naives, support vectors machines, and
K-nearest neighbors classifiers to the digits dataset. Now that we
have these validation tools in place, we can ask quantitatively which
of the three estimators works best for this dataset.</p>
</div>
<ul>
<li><p class="first">With the default hyper-parameters for each estimator, which gives the
best f1 score on the <strong>validation set</strong>?  Recall that hyperparameters
are the parameters set when you instantiate the classifier: for
example, the <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> in <code class="docutils literal notranslate"><span class="pre">clf</span> <span class="pre">=</span>
<span class="pre">KNeighborsClassifier(n_neighbors=1)</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
<div class="newline"></div><span class="gp">... </span>                            <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">Model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">GaussianNB</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">,</span> <span class="n">LinearSVC</span><span class="p">]:</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">clf</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<div class="newline"></div><span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
<div class="newline"></div><span class="gp">... </span>          <span class="p">(</span><span class="n">Model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">)))</span>  
<div class="newline"></div><span class="go">GaussianNB: 0.8...</span>
<div class="newline"></div><span class="go">KNeighborsClassifier: 0.9...</span>
<div class="newline"></div><span class="go">LinearSVC: 0.9...</span>
<div class="newline"></div></pre></div>
</div>
</li>
<li><p class="first">For each classifier, which value for the hyperparameters gives the best
results for the digits data?  For <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code></a>, use
<code class="docutils literal notranslate"><span class="pre">loss='l2'</span></code> and <code class="docutils literal notranslate"><span class="pre">loss='l1'</span></code>.  For
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> we use
<code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> between 1 and 10. Note that
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">GaussianNB</span></code></a> does not have any adjustable
hyperparameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">):</span> <span class="mf">0.930570687535</span>
<div class="newline"></div><span class="n">LinearSVC</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">):</span> <span class="mf">0.933068826918</span>
<div class="newline"></div><span class="o">-------------------</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span> <span class="mf">0.991367521884</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span> <span class="mf">0.984844206884</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span> <span class="mf">0.986775344954</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span> <span class="mf">0.980371905382</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span> <span class="mf">0.980456280495</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span> <span class="mf">0.975792419414</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span> <span class="mf">0.978064579214</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span> <span class="mf">0.978064579214</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">9</span><span class="p">):</span> <span class="mf">0.978064579214</span>
<div class="newline"></div><span class="n">KNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span> <span class="mf">0.975555089773</span>
<div class="newline"></div></pre></div>
</div>
<p><strong>Solution:</strong> <a class="reference internal" href="auto_examples/plot_compare_classifiers.html#sphx-glr-packages-scikit-learn-auto-examples-plot-compare-classifiers-py"><span class="std std-ref">code source</span></a></p>
</li>
</ul>
</div>
<div class="section" id="cross-validation">
<h3>3.6.5.4. Cross-validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h3>
<p>Cross-validation consists in repetively splitting the data in pairs of
train and test sets, called ‘folds’. Scikit-learn comes with a function
to automatically compute score on all these folds. Here we do
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code></a> with k=5.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> 
<div class="newline"></div><span class="go">array([0.947...,  0.955...,  0.966...,  0.980...,  0.963... ])</span>
<div class="newline"></div></pre></div>
</div>
<p>We can use different splitting strategies, such as random splitting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>  
<div class="newline"></div><span class="go">array([...])</span>
<div class="newline"></div></pre></div>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">There exists <a class="reference external" href="http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators">many different cross-validation strategies</a>
in scikit-learn. They are often useful to take in account non iid
datasets.</p>
</div>
</div>
<div class="section" id="hyperparameter-optimization-with-cross-validation">
<h3>3.6.5.5. Hyperparameter optimization with cross-validation<a class="headerlink" href="#hyperparameter-optimization-with-cross-validation" title="Permalink to this headline">¶</a></h3>
<p>Consider regularized linear models, such as <em>Ridge Regression</em>, which
uses l2 regularlization, and <em>Lasso Regression</em>, which uses l1
regularization. Choosing their regularization parameter is important.</p>
<p>Let us set these parameters on the Diabetes dataset, a simple regression
problem. The diabetes data consists of 10 physiological variables (age,
sex, weight, blood pressure) measure on 442 patients, and an indication
of disease progression after one year:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div><span class="go">(442, 10)</span>
<div class="newline"></div></pre></div>
</div>
<p>With the default hyper-parameters: we compute the cross-validation score:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">Model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">]:</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<div class="newline"></div><span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">Model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
<div class="newline"></div><span class="go">Ridge: 0.4...</span>
<div class="newline"></div><span class="go">Lasso: 0.3...</span>
<div class="newline"></div></pre></div>
</div>
<div class="section" id="basic-hyperparameter-optimization">
<h4>Basic Hyperparameter Optimization<a class="headerlink" href="#basic-hyperparameter-optimization" title="Permalink to this headline">¶</a></h4>
<p>We compute the cross-validation score as a function of alpha, the
strength of the regularization for <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a>
and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a>. We choose 20 values of alpha
between 0.0001 and 1:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">Model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Lasso</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">]:</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">Model</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<div class="newline"></div><span class="gp">... </span>              <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">]</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">Model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span> 
<div class="newline"></div><span class="go">[&lt;matplotlib.lines.Line2D object at ...</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_linear_model_cv.html"><img alt="../../_images/sphx_glr_plot_linear_model_cv_001.png" class="align-left" src="../../_images/sphx_glr_plot_linear_model_cv_001.png" style="width: 350.0px; height: 210.0px;" /></a>
<div class="green topic">
<p class="topic-title">Question</p>
<p>Can we trust our results to be actually useful?</p>
</div>
</div>
<div class="section" id="automatically-performing-grid-search">
<h4>Automatically Performing Grid Search<a class="headerlink" href="#automatically-performing-grid-search" title="Permalink to this headline">¶</a></h4>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.grid_search.GridSearchCV</span></code> is constructed with an
estimator, as well as a dictionary of parameter values to be searched.
We can find the optimal parameters this way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">Model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">]:</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">gscv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">Model</span><span class="p">(),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alphas</span><span class="p">),</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div><span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">Model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">gscv</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
<div class="newline"></div><span class="go">Ridge: {&#39;alpha&#39;: 0.062101694189156162}</span>
<div class="newline"></div><span class="go">Lasso: {&#39;alpha&#39;: 0.01268961003167922}</span>
<div class="newline"></div></pre></div>
</div>
</div>
<div class="section" id="built-in-hyperparameter-search">
<h4>Built-in Hyperparameter Search<a class="headerlink" href="#built-in-hyperparameter-search" title="Permalink to this headline">¶</a></h4>
<p>For some models within scikit-learn, cross-validation can be performed
more efficiently on large datasets.  In this case, a cross-validated
version of the particular model is included.  The cross-validated
versions of <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> and
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a> are
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">RidgeCV</span></code></a> and
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoCV</span></code></a>, respectively.  Parameter search
on these estimators can be performed as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span><span class="p">,</span> <span class="n">LassoCV</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">Model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">RidgeCV</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">]:</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div><span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">Model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">alpha_</span><span class="p">))</span>
<div class="newline"></div><span class="go">RidgeCV: 0.0621016941892</span>
<div class="newline"></div><span class="go">LassoCV: 0.0126896100317</span>
<div class="newline"></div></pre></div>
</div>
<p>We see that the results match those returned by GridSearchCV</p>
</div>
<div class="section" id="nested-cross-validation">
<h4>Nested cross-validation<a class="headerlink" href="#nested-cross-validation" title="Permalink to this headline">¶</a></h4>
<p>How do we measure the performance of these estimators? We have used data
to set the hyperparameters, so we need to test on actually new data. We
can do this by running <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="(in scikit-learn v1.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">cross_val_score()</span></code></a>
on our CV objects. Here there are 2 cross-validation loops going on, this
is called <em>‘nested cross validation’</em>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">Model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">RidgeCV</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">]:</span>
<div class="newline"></div>    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">Model</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<div class="newline"></div>    <span class="nb">print</span><span class="p">(</span><span class="n">Model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
<div class="newline"></div></pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Note that these results do not match the best results of our curves
above, and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoCV</span></code></a> seems to
under-perform <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">RidgeCV</span></code></a>. The reason is
that setting the hyper-parameter is harder for Lasso, thus the
estimation error on this hyper-parameter is larger.</p>
</div>
</div>
</div>
</div>
<div class="section" id="unsupervised-learning-dimensionality-reduction-and-visualization">
<h2><a class="toc-backref" href="#id30">3.6.6. Unsupervised Learning: Dimensionality Reduction and Visualization</a><a class="headerlink" href="#unsupervised-learning-dimensionality-reduction-and-visualization" title="Permalink to this headline">¶</a></h2>
<p>Unsupervised learning is applied on X without y: data without labels. A
typical use case is to find hidden structure in the data.</p>
<div class="section" id="dimensionality-reduction-pca">
<h3>3.6.6.1. Dimensionality Reduction: PCA<a class="headerlink" href="#dimensionality-reduction-pca" title="Permalink to this headline">¶</a></h3>
<p>Dimensionality reduction derives a set of new artificial features smaller
than the original feature set. Here we’ll use <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component
Analysis (PCA)</a>, a
dimensionality reduction that strives to retain most of the variance of
the original data. We’ll use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.decomposition.PCA</span></code></a> on the
iris dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<div class="newline"></div></pre></div>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last"><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> computes linear combinations of
the original features using a truncated Singular Value Decomposition
of the matrix X, to project the data onto a base of the top singular
vectors.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<div class="newline"></div><span class="go">PCA(n_components=2, ...)</span>
<div class="newline"></div></pre></div>
</div>
<p>Once fitted, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> exposes the singular
vectors in the <code class="docutils literal notranslate"><span class="pre">components_</span></code> attribute:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span>     
<div class="newline"></div><span class="go">array([[ 0.3..., -0.08...,  0.85...,  0.3...],</span>
<div class="newline"></div><span class="go">       [ 0.6...,  0.7..., -0.1..., -0.07...]])</span>
<div class="newline"></div></pre></div>
</div>
<p>Other attributes are available as well:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>    
<div class="newline"></div><span class="go">array([0.92...,  0.053...])</span>
<div class="newline"></div></pre></div>
</div>
<p>Let us project the iris dataset along those first two dimensions::</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X_pca</span><span class="o">.</span><span class="n">shape</span>
<div class="newline"></div><span class="go">(150, 2)</span>
<div class="newline"></div></pre></div>
</div>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> <code class="docutils literal notranslate"><span class="pre">normalizes</span></code> and <code class="docutils literal notranslate"><span class="pre">whitens</span></code> the data, which means that the data
is now centered on both components with unit variance:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_pca</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
<div class="newline"></div><span class="go">array([...e-15,  ...e-15])</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X_pca</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<div class="newline"></div><span class="go">array([1.,  1.])</span>
<div class="newline"></div></pre></div>
</div>
<p>Furthermore, the samples components do no longer carry any linear
correlation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">X_pca</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  
<div class="newline"></div><span class="go">array([[1.00000000e+00,   0.0],</span>
<div class="newline"></div><span class="go">       [0.0,   1.00000000e+00]])</span>
<div class="newline"></div></pre></div>
</div>
<p>With a number of retained components 2 or 3, PCA is useful to visualize
the dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">target_ids</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">))</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">target_ids</span><span class="p">,</span> <span class="s1">&#39;rgbcmykw&#39;</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<div class="newline"></div><span class="gp">... </span>                <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span> 
<div class="newline"></div><span class="go">&lt;matplotlib.collections.PathCollection ...</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_pca.html"><img alt="../../_images/sphx_glr_plot_pca_001.png" class="align-left" src="../../_images/sphx_glr_plot_pca_001.png" style="width: 420.0px; height: 350.0px;" /></a>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Note that this projection was determined <em>without</em> any information
about the labels (represented by the colors): this is the sense in
which the learning is <strong>unsupervised</strong>. Nevertheless, we see that the
projection gives us insight into the distribution of the different
flowers in parameter space: notably, <em>iris setosa</em> is much more
distinct than the other two species.</p>
</div>
</div>
<div class="section" id="visualization-with-a-non-linear-embedding-tsne">
<h3>3.6.6.2. Visualization with a non-linear embedding: tSNE<a class="headerlink" href="#visualization-with-a-non-linear-embedding-tsne" title="Permalink to this headline">¶</a></h3>
<p>For visualization, more complex embeddings can be useful (for statistical
analysis, they are harder to control). <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.manifold.TSNE</span></code></a> is
such a powerful manifold learning method. We apply it to the <em>digits</em>
dataset, as the digits are vectors of dimension 8*8 = 64. Embedding them
in 2D enables visualization:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Take the first 500 data points: it&#39;s hard to see 1500 points</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Fit and transform with a TSNE</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">X_2d</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Visualize the data</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span> 
<div class="newline"></div><span class="go">&lt;matplotlib.collections.PathCollection object at ...&gt;</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_tsne.html"><img alt="../../_images/sphx_glr_plot_tsne_001.png" class="align-left" src="../../_images/sphx_glr_plot_tsne_001.png" style="width: 420.0px; height: 350.0px;" /></a>
<div class="topic">
<p class="topic-title">fit_transform</p>
<p>As <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TSNE</span></code></a> cannot be applied to new data, we
need to use its <cite>fit_transform</cite> method.</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.manifold.TSNE</span></code></a> separates quite well the different classes
of digits eventhough it had no access to the class information.</p>
<div style="clear: both"></div><div class="green topic">
<p class="topic-title">Exercise: Other dimension reduction of digits</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold" title="(in scikit-learn v1.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.manifold</span></code></a> has many other non-linear embeddings. Try
them out on the digits dataset. Could you judge their quality without
knowing the labels <code class="docutils literal notranslate"><span class="pre">y</span></code>?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># ...</span>
<div class="newline"></div></pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="the-eigenfaces-example-chaining-pca-and-svms">
<h2><a class="toc-backref" href="#id31">3.6.7. The eigenfaces example: chaining PCA and SVMs</a><a class="headerlink" href="#the-eigenfaces-example-chaining-pca-and-svms" title="Permalink to this headline">¶</a></h2>
<div class="sidebar">
<p class="first sidebar-title">Code and notebook</p>
<p class="last">Python code and Jupyter notebook for this section are found
<a class="reference internal" href="auto_examples/plot_eigenfaces.html#sphx-glr-packages-scikit-learn-auto-examples-plot-eigenfaces-py"><span class="std std-ref">here</span></a></p>
</div>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id32">3.6.8. The eigenfaces example: chaining PCA and SVMs</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>The goal of this example is to show how an unsupervised method and a
supervised one can be chained for better prediction. It starts with a
didactic but lengthy way of doing things, and finishes with the
idiomatic approach to pipelining in scikit-learn.</p>
<p>Here we’ll take a look at a simple facial recognition example. Ideally,
we would use a dataset consisting of a subset of the <a class="reference external" href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in
the Wild</a> data that is available
with <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html#sklearn.datasets.fetch_lfw_people" title="(in scikit-learn v1.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.datasets.fetch_lfw_people()</span></code></a>. However, this is a
relatively large download (~200MB) so we will do the tutorial on a
simpler, less rich dataset. Feel free to explore the LFW dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<div class="newline"></div><span class="n">faces</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_olivetti_faces</span><span class="p">()</span>
<div class="newline"></div><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
<div class="newline"></div></pre></div>
</div>
<p>Let’s visualize these faces to see what we’re working with</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<div class="newline"></div><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<div class="newline"></div><span class="c1"># plot several images</span>
<div class="newline"></div><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">15</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_eigenfaces_001.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_eigenfaces_001.png" />
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Note is that these faces have already been localized and scaled to a
common size. This is an important preprocessing piece for facial
recognition, and is a process that can require a large collection of
training data. This can be done in scikit-learn, but the challenge is
gathering a sufficient amount of training data for the algorithm to work.
Fortunately, this piece is common enough that it has been done. One good
resource is
<a class="reference external" href="https://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html">OpenCV</a>,
the <em>Open Computer Vision Library</em>.</p>
</div>
<p>We’ll perform a Support Vector classification of the images. We’ll do a
typical train-test split on the images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<div class="newline"></div><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
<div class="newline"></div>        <span class="n">faces</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(300, 4096) (100, 4096)
<div class="newline"></div></pre></div>
</div>
<div class="section" id="preprocessing-principal-component-analysis">
<h3>3.6.8.1. Preprocessing: Principal Component Analysis<a class="headerlink" href="#preprocessing-principal-component-analysis" title="Permalink to this headline">¶</a></h3>
<p>1850 dimensions is a lot for SVM. We can use PCA to reduce these 1850
features to a manageable size, while maintaining most of the information
in the dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<div class="newline"></div><span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<div class="newline"></div><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p>One interesting part of PCA is that it computes the “mean” face, which
can be interesting to examine:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
<div class="newline"></div>           <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_eigenfaces_002.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_eigenfaces_002.png" />
<p>The principal components measure deviations about this mean along
orthogonal axes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(150, 4096)
<div class="newline"></div></pre></div>
</div>
<p>It is also interesting to visualize these principal components:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<div class="newline"></div><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
<div class="newline"></div>              <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_eigenfaces_003.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_eigenfaces_003.png" />
<p>The components (“eigenfaces”) are ordered by their importance from
top-left to bottom-right. We see that the first few components seem to
primarily take care of lighting conditions; the remaining components
pull out certain identifying features: the nose, eyes, eyebrows, etc.</p>
<p>With this projection computed, we can now project our original training
and test data onto the PCA basis:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<div class="newline"></div><span class="n">X_test_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><span class="n">X_train_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(300, 150)
<div class="newline"></div></pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_test_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(100, 150)
<div class="newline"></div></pre></div>
</div>
<p>These projected components correspond to factors in a linear combination
of component images such that the combination approaches the original
face.</p>
</div>
<div class="section" id="doing-the-learning-support-vector-machines">
<h3>3.6.8.2. Doing the Learning: Support Vector Machines<a class="headerlink" href="#doing-the-learning-support-vector-machines" title="Permalink to this headline">¶</a></h3>
<p>Now we’ll perform support-vector-machine classification on this reduced
dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<div class="newline"></div><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<div class="newline"></div><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p>Finally, we can evaluate how well this classification did. First, we
might plot a few of the test-cases with the labels learned from the
training set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<div class="newline"></div><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<div class="newline"></div><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">15</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
<div class="newline"></div>              <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<div class="newline"></div>    <span class="n">color</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;black&#39;</span> <span class="k">if</span> <span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;small&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_eigenfaces_004.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_eigenfaces_004.png" />
<p>The classifier is correct on an impressive number of images given the
simplicity of its learning model! Using a linear classifier on 150
features derived from the pixel-level data, the algorithm correctly
identifies a large number of the people in the images.</p>
<p>Again, we can quantify this effectiveness using one of several measures
from <a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics" title="(in scikit-learn v1.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a>. First we can do the classification
report, which shows the precision, recall and other measures of the
“goodness” of the classification:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<div class="newline"></div><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>precision    recall  f1-score   support
<div class="newline"></div>
<div class="newline"></div>           0       1.00      0.50      0.67         6
<div class="newline"></div>           1       1.00      1.00      1.00         4
<div class="newline"></div>           2       0.50      1.00      0.67         2
<div class="newline"></div>           3       1.00      1.00      1.00         1
<div class="newline"></div>           4       0.33      1.00      0.50         1
<div class="newline"></div>           5       1.00      1.00      1.00         5
<div class="newline"></div>           6       1.00      1.00      1.00         4
<div class="newline"></div>           7       1.00      0.67      0.80         3
<div class="newline"></div>           9       1.00      1.00      1.00         1
<div class="newline"></div>          10       1.00      1.00      1.00         4
<div class="newline"></div>          11       1.00      1.00      1.00         1
<div class="newline"></div>          12       0.67      1.00      0.80         2
<div class="newline"></div>          13       1.00      1.00      1.00         3
<div class="newline"></div>          14       1.00      1.00      1.00         5
<div class="newline"></div>          15       1.00      1.00      1.00         3
<div class="newline"></div>          17       1.00      1.00      1.00         6
<div class="newline"></div>          19       1.00      1.00      1.00         4
<div class="newline"></div>          20       1.00      1.00      1.00         1
<div class="newline"></div>          21       1.00      1.00      1.00         1
<div class="newline"></div>          22       1.00      1.00      1.00         2
<div class="newline"></div>          23       1.00      1.00      1.00         1
<div class="newline"></div>          24       1.00      1.00      1.00         2
<div class="newline"></div>          25       1.00      0.50      0.67         2
<div class="newline"></div>          26       1.00      0.75      0.86         4
<div class="newline"></div>          27       1.00      1.00      1.00         1
<div class="newline"></div>          28       0.67      1.00      0.80         2
<div class="newline"></div>          29       1.00      1.00      1.00         3
<div class="newline"></div>          30       1.00      1.00      1.00         4
<div class="newline"></div>          31       1.00      1.00      1.00         3
<div class="newline"></div>          32       1.00      1.00      1.00         3
<div class="newline"></div>          33       1.00      1.00      1.00         2
<div class="newline"></div>          34       1.00      1.00      1.00         3
<div class="newline"></div>          35       1.00      1.00      1.00         1
<div class="newline"></div>          36       1.00      1.00      1.00         3
<div class="newline"></div>          37       1.00      1.00      1.00         3
<div class="newline"></div>          38       1.00      1.00      1.00         1
<div class="newline"></div>          39       1.00      1.00      1.00         3
<div class="newline"></div>
<div class="newline"></div>    accuracy                           0.94       100
<div class="newline"></div>   macro avg       0.95      0.96      0.94       100
<div class="newline"></div>weighted avg       0.97      0.94      0.94       100
<div class="newline"></div></pre></div>
</div>
<p>Another interesting metric is the <em>confusion matrix</em>, which indicates
how often any two items are mixed-up. The confusion matrix of a perfect
classifier would only have nonzero entries on the diagonal, with zeros
on the off-diagonal:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[3 0 0 ... 0 0 0]
<div class="newline"></div> [0 4 0 ... 0 0 0]
<div class="newline"></div> [0 0 2 ... 0 0 0]
<div class="newline"></div> ...
<div class="newline"></div> [0 0 0 ... 3 0 0]
<div class="newline"></div> [0 0 0 ... 0 1 0]
<div class="newline"></div> [0 0 0 ... 0 0 3]]
<div class="newline"></div></pre></div>
</div>
</div>
<div class="section" id="pipelining">
<h3>3.6.8.3. Pipelining<a class="headerlink" href="#pipelining" title="Permalink to this headline">¶</a></h3>
<p>Above we used PCA as a pre-processing step before applying our support
vector machine classifier. Plugging the output of one estimator directly
into the input of a second estimator is a commonly used pattern; for
this reason scikit-learn provides a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> object which automates
this process. The above problem can be re-expressed as a pipeline as
follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<div class="newline"></div><span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;pca&#39;</span><span class="p">,</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">)),</span>
<div class="newline"></div>                <span class="p">(</span><span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))])</span>
<div class="newline"></div>
<div class="newline"></div><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<div class="newline"></div></pre></div>
</div>
</div>
</div>
<div class="section" id="parameter-selection-validation-and-testing">
<h2><a class="toc-backref" href="#id33">3.6.9. Parameter selection, Validation, and Testing</a><a class="headerlink" href="#parameter-selection-validation-and-testing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="hyperparameters-over-fitting-and-under-fitting">
<h3>3.6.9.1. Hyperparameters, Over-fitting, and Under-fitting<a class="headerlink" href="#hyperparameters-over-fitting-and-under-fitting" title="Permalink to this headline">¶</a></h3>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">This section is adapted from <a class="reference external" href="https://www.coursera.org/course/ml">Andrew Ng’s excellent
Coursera course</a></p>
</div>
<p>The issues associated with validation and cross-validation are some of
the most important aspects of the practice of machine learning.
Selecting the optimal model for your data is vital, and is a piece of
the problem that is not often appreciated by machine learning
practitioners.</p>
<p>The central question is: <strong>If our estimator is underperforming, how
should we move forward?</strong></p>
<ul class="simple">
<li>Use simpler or more complicated model?</li>
<li>Add more features to each observed data point?</li>
<li>Add more training samples?</li>
</ul>
<p>The answer is often counter-intuitive. In particular, <strong>Sometimes using
a more complicated model will give worse results.</strong> Also, <strong>Sometimes
adding training data will not improve your results.</strong> The ability to
determine what steps will improve your model is what separates the
successful machine learning practitioners from the unsuccessful.</p>
<div class="section" id="bias-variance-trade-off-illustration-on-a-simple-regression-problem">
<h4>Bias-variance trade-off: illustration on a simple regression problem<a class="headerlink" href="#bias-variance-trade-off-illustration-on-a-simple-regression-problem" title="Permalink to this headline">¶</a></h4>
<div class="sidebar">
<p class="first sidebar-title">Code and notebook</p>
<p class="last">Python code and Jupyter notebook for this section are found
<a class="reference internal" href="auto_examples/plot_variance_linear_regr.html#sphx-glr-packages-scikit-learn-auto-examples-plot-variance-linear-regr-py"><span class="std std-ref">here</span></a></p>
</div>
<p>Let us start with a simple 1D regression problem. This
will help us to easily visualize the data and the model, and the results
generalize easily to higher-dimensional datasets. We’ll explore a simple
<strong>linear regression</strong> problem, with <a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model" title="(in scikit-learn v1.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span> <span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
<div class="newline"></div><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<div class="newline"></div><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
<div class="newline"></div></pre></div>
</div>
<p>Without noise, as linear regression fits the data perfectly</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<div class="newline"></div><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<div class="newline"></div><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<div class="newline"></div><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_variance_linear_regr_001.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_variance_linear_regr_001.png" />
<p>In real life situation, we have noise (e.g. measurement noise) in our data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<div class="newline"></div><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">noisy_X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">noisy_X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">noisy_X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_variance_linear_regr_002.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_variance_linear_regr_002.png" />
<p>As we can see, our linear model captures and amplifies the noise in the
data. It displays a lot of variance.</p>
<p>We can use another linear estimator that uses regularization, the
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> estimator. This estimator
regularizes the coefficients by shrinking them to zero, under the
assumption that very high correlations are often spurious. The alpha
parameter controls the amount of shrinkage used.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<div class="newline"></div><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<div class="newline"></div><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">noisy_X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">noisy_X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">noisy_X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<div class="newline"></div>
<div class="newline"></div><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_variance_linear_regr_003.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_variance_linear_regr_003.png" />
<p>As we can see, the estimator displays much less variance. However it
systematically under-estimates the coefficient. It displays a biased
behavior.</p>
<p>This is a typical example of <strong>bias/variance tradeof</strong>: non-regularized
estimator are not biased, but they can display a lot of variance.
Highly-regularized models have little variance, but high bias. This bias
is not necessarily a bad thing: what matters is choosing the
tradeoff between bias and variance that leads to the best prediction
performance. For a specific dataset there is a sweet spot corresponding
to the highest complexity that the data can support, depending on the
amount of noise and of observations available.</p>
</div>
</div>
<div class="section" id="visualizing-the-bias-variance-tradeoff">
<h3>3.6.9.2. Visualizing the Bias/Variance Tradeoff<a class="headerlink" href="#visualizing-the-bias-variance-tradeoff" title="Permalink to this headline">¶</a></h3>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Given a particular dataset and a model (e.g. a polynomial), we’d like to
understand whether bias (underfit) or variance limits prediction, and how
to tune the <em>hyperparameter</em> (here <code class="docutils literal notranslate"><span class="pre">d</span></code>, the degree of the polynomial)
to give the best fit.</p>
</div>
<p>On a given data, let us fit a simple polynomial regression model with
varying degrees:</p>
<a class="reference external image-reference" href="auto_examples/plot_bias_variance.html"><img alt="../../_images/sphx_glr_plot_bias_variance_001.png" class="align-center" src="../../_images/sphx_glr_plot_bias_variance_001.png" /></a>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p>In the above figure, we see fits for three different values of <code class="docutils literal notranslate"><span class="pre">d</span></code>.
For <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">1</span></code>, the data is under-fit. This means that the model is too
simplistic: no straight line will ever be a good fit to this data. In
this case, we say that the model suffers from high bias. The model
itself is biased, and this will be reflected in the fact that the data
is poorly fit. At the other extreme, for <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">6</span></code> the data is over-fit.
This means that the model has too many free parameters (6 in this case)
which can be adjusted to perfectly fit the training data. If we add a
new point to this plot, though, chances are it will be very far from the
curve representing the degree-6 fit. In this case, we say that the model
suffers from high variance. The reason for the term “high variance” is
that if any of the input points are varied slightly, it could result in
a very different model.</p>
<p class="last">In the middle, for <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">2</span></code>, we have found a good mid-point. It fits
the data fairly well, and does not suffer from the bias and variance
problems seen in the figures on either side. What we would like is a way
to quantitatively identify bias and variance, and optimize the
metaparameters (in this case, the polynomial degree d) in order to
determine the best algorithm.</p>
</div>
<div class="topic">
<p class="topic-title">Polynomial regression with scikit-learn</p>
<p>A polynomial regression is built by pipelining
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">PolynomialFeatures</span></code></a>
and a <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<div class="newline"></div></pre></div>
</div>
</div>
<div class="section" id="validation-curves">
<h4>Validation Curves<a class="headerlink" href="#validation-curves" title="Permalink to this headline">¶</a></h4>
<p>Let us create a dataset like in the example above:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">generating_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">err</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span> <span class="o">-</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">),</span> <span class="n">err</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># randomly sample more data</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">generating_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">err</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_bias_variance.html"><img alt="../../_images/sphx_glr_plot_bias_variance_002.png" class="align-right" src="../../_images/sphx_glr_plot_bias_variance_002.png" style="width: 360.0px; height: 240.0px;" /></a>
<p>Central to quantify bias and variance of a model is to apply it on <em>test
data</em>, sampled from the same distribution as the train, but that will
capture independent noise:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xtrain</span><span class="p">,</span> <span class="n">xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<div style="clear: both"></div><p><strong>Validation curve</strong> A validation curve consists in varying a model parameter
that controls its complexity (here the degree of the
polynomial) and measures both error of the model on training data, and on
test data (<em>eg</em> with cross-validation). The model parameter is then
adjusted so that the test error is minimized:</p>
<p>We use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html#sklearn.model_selection.validation_curve" title="(in scikit-learn v1.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.model_selection.validation_curve()</span></code></a> to compute train
and test error, and plot it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">validation_curve</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">degrees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Vary the &quot;degrees&quot; on the pipeline step &quot;polynomialfeatures&quot;</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">train_scores</span><span class="p">,</span> <span class="n">validation_scores</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span>
<div class="newline"></div><span class="gp">... </span>                <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span>
<div class="newline"></div><span class="gp">... </span>                <span class="n">param_name</span><span class="o">=</span><span class="s1">&#39;polynomialfeatures__degree&#39;</span><span class="p">,</span>
<div class="newline"></div><span class="gp">... </span>                <span class="n">param_range</span><span class="o">=</span><span class="n">degrees</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Plot the mean train score and validation score across folds</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">validation_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;cross-validation&#39;</span><span class="p">)</span>  
<div class="newline"></div><span class="go">[&lt;matplotlib.lines.Line2D object at ...&gt;]</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">train_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>  
<div class="newline"></div><span class="go">[&lt;matplotlib.lines.Line2D object at ...&gt;]</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>  
<div class="newline"></div><span class="go">&lt;matplotlib.legend.Legend object at ...&gt;</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_bias_variance.html"><img alt="../../_images/sphx_glr_plot_bias_variance_003.png" class="align-left" src="../../_images/sphx_glr_plot_bias_variance_003.png" style="width: 360.0px; height: 240.0px;" /></a>
<p>This figure shows why validation is important. On the left side of the
plot, we have very low-degree polynomial, which under-fit the data. This
leads to a low explained variance for both the training set and the
validation set. On the far right side of the plot, we have a very high
degree polynomial, which over-fits the data. This can be seen in the fact
that the training explained variance is very high, while on the
validation set, it is low. Choosing <code class="docutils literal notranslate"><span class="pre">d</span></code> around 4 or 5 gets us the best
tradeoff.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">The astute reader will realize that something is amiss here: in the
above plot, <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">4</span></code> gives the best results. But in the previous plot,
we found that <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">6</span></code> vastly over-fits the data. What’s going on here?
The difference is the <strong>number of training points</strong> used. In the
previous example, there were only eight training points. In this
example, we have 100. As a general rule of thumb, the more training
points used, the more complicated model can be used. But how can you
determine for a given model whether more training points will be
helpful? A useful diagnostic for this are learning curves.</p>
</div>
</div>
<div class="section" id="learning-curves">
<h4>Learning Curves<a class="headerlink" href="#learning-curves" title="Permalink to this headline">¶</a></h4>
<p>A learning curve shows the training and validation score as a
function of the number of training points. Note that when we train on a
subset of the training data, the training score is computed using
this subset, not the full training set. This curve gives a
quantitative view into how beneficial it will be to add training
samples.</p>
<div class="green topic">
<p class="topic-title"><strong>Questions:</strong></p>
<ul class="simple">
<li>As the number of training samples are increased, what do you expect
to see for the training score? For the validation score?</li>
<li>Would you expect the training score to be higher or lower than the
validation score? Would you ever expect this to change?</li>
</ul>
</div>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">scikit-learn</span></code> provides
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve" title="(in scikit-learn v1.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.model_selection.learning_curve()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">learning_curve</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">validation_scores</span> <span class="o">=</span> <span class="n">learning_curve</span><span class="p">(</span>
<div class="newline"></div><span class="gp">... </span>    <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_sizes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Plot the mean train score and validation score across folds</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">validation_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;cross-validation&#39;</span><span class="p">)</span> 
<div class="newline"></div><span class="go">[&lt;matplotlib.lines.Line2D object at ...&gt;]</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span> 
<div class="newline"></div><span class="go">[&lt;matplotlib.lines.Line2D object at ...&gt;]</span>
<div class="newline"></div></pre></div>
</div>
<div class="figure align-left" id="id6">
<a class="reference external image-reference" href="auto_examples/plot_bias_variance.html"><img alt="../../_images/sphx_glr_plot_bias_variance_004.png" src="../../_images/sphx_glr_plot_bias_variance_004.png" style="width: 360.0px; height: 240.0px;" /></a>
<p class="caption"><span class="caption-text">For a <code class="docutils literal notranslate"><span class="pre">degree=1</span></code> model</span></p>
</div>
<p>Note that the validation score <em>generally increases</em> with a growing
training set, while the training score <em>generally decreases</em> with a
growing training set. As the training size
increases, they will converge to a single value.</p>
<p>From the above discussion, we know that <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">1</span></code> is a high-bias
estimator which under-fits the data. This is indicated by the fact that
both the training and validation scores are low. When confronted
with this type of learning curve, we can expect that adding more
training data will not help: both lines converge to a
relatively low score.</p>
<p><div style="clear: both"></div></p>
<p><strong>When the learning curves have converged to a low score, we have a
high bias model.</strong></p>
<p>A high-bias model can be improved by:</p>
<ul class="simple">
<li>Using a more sophisticated model (i.e. in this case, increase <code class="docutils literal notranslate"><span class="pre">d</span></code>)</li>
<li>Gather more features for each sample.</li>
<li>Decrease regularization in a regularized model.</li>
</ul>
<p>Increasing the number of samples, however, does not improve a high-bias
model.</p>
<p>Now let’s look at a high-variance (i.e. over-fit) model:</p>
<div class="figure align-left" id="id7">
<a class="reference external image-reference" href="auto_examples/plot_bias_variance.html"><img alt="../../_images/sphx_glr_plot_bias_variance_006.png" src="../../_images/sphx_glr_plot_bias_variance_006.png" style="width: 360.0px; height: 240.0px;" /></a>
<p class="caption"><span class="caption-text">For a <code class="docutils literal notranslate"><span class="pre">degree=15</span></code> model</span></p>
</div>
<p>Here we show the learning curve for <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">15</span></code>. From the above
discussion, we know that <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">15</span></code> is a <strong>high-variance</strong> estimator
which <strong>over-fits</strong> the data. This is indicated by the fact that the
training score is much higher than the validation score. As we add more
samples to this training set, the training score will continue to
decrease, while the cross-validation error will continue to increase, until they
meet in the middle.</p>
<p><div style="clear: both"></div></p>
<p><strong>Learning curves that have not yet converged with the full training
set indicate a high-variance, over-fit model.</strong></p>
<p>A high-variance model can be improved by:</p>
<ul class="simple">
<li>Gathering more training samples.</li>
<li>Using a less-sophisticated model (i.e. in this case, make <code class="docutils literal notranslate"><span class="pre">d</span></code>
smaller)</li>
<li>Increasing regularization.</li>
</ul>
<p>In particular, gathering more features for each sample will not help the
results.</p>
</div>
</div>
<div class="section" id="summary-on-model-selection">
<h3>3.6.9.3. Summary on model selection<a class="headerlink" href="#summary-on-model-selection" title="Permalink to this headline">¶</a></h3>
<p>We’ve seen above that an under-performing algorithm can be due to two
possible situations: high bias (under-fitting) and high variance
(over-fitting). In order to evaluate our algorithm, we set aside a
portion of our training data for cross-validation. Using the technique
of learning curves, we can train on progressively larger subsets of the
data, evaluating the training error and cross-validation error to
determine whether our algorithm has high variance or high bias. But what
do we do with this information?</p>
<div class="section" id="high-bias">
<h4>High Bias<a class="headerlink" href="#high-bias" title="Permalink to this headline">¶</a></h4>
<p>If a model shows high <strong>bias</strong>, the following actions might help:</p>
<ul class="simple">
<li><strong>Add more features</strong>. In our example of predicting home prices, it
may be helpful to make use of information such as the neighborhood
the house is in, the year the house was built, the size of the lot,
etc. Adding these features to the training and test sets can improve
a high-bias estimator</li>
<li><strong>Use a more sophisticated model</strong>. Adding complexity to the model
can help improve on bias. For a polynomial fit, this can be
accomplished by increasing the degree d. Each learning technique has
its own methods of adding complexity.</li>
<li><strong>Use fewer samples</strong>. Though this will not improve the
classification, a high-bias algorithm can attain nearly the same
error with a smaller training sample. For algorithms which are
computationally expensive, reducing the training sample size can lead
to very large improvements in speed.</li>
<li><strong>Decrease regularization</strong>. Regularization is a technique used to
impose simplicity in some machine learning models, by adding a
penalty term that depends on the characteristics of the parameters.
If a model has high bias, decreasing the effect of regularization can
lead to better results.</li>
</ul>
</div>
<div class="section" id="high-variance">
<h4>High Variance<a class="headerlink" href="#high-variance" title="Permalink to this headline">¶</a></h4>
<p>If a model shows <strong>high variance</strong>, the following actions might
help:</p>
<ul class="simple">
<li><strong>Use fewer features</strong>. Using a feature selection technique may be
useful, and decrease the over-fitting of the estimator.</li>
<li><strong>Use a simpler model</strong>. Model complexity and over-fitting go
hand-in-hand.</li>
<li><strong>Use more training samples</strong>. Adding training samples can reduce the
effect of over-fitting, and lead to improvements in a high variance
estimator.</li>
<li><strong>Increase Regularization</strong>. Regularization is designed to prevent
over-fitting. In a high-variance model, increasing regularization can
lead to better results.</li>
</ul>
<p>These choices become very important in real-world situations. For
example, due to limited telescope time, astronomers must seek a balance
between observing a large number of objects, and observing a large
number of features for each object. Determining which is more important
for a particular learning task can inform the observing strategy that
the astronomer employs.</p>
</div>
</div>
<div class="section" id="a-last-word-of-caution-separate-validation-and-test-set">
<h3>3.6.9.4. A last word of caution: separate validation and test set<a class="headerlink" href="#a-last-word-of-caution-separate-validation-and-test-set" title="Permalink to this headline">¶</a></h3>
<p>Using validation schemes to determine hyper-parameters means that we are
fitting the hyper-parameters to the particular validation set. In the
same way that parameters can be over-fit to the training set,
hyperparameters can be over-fit to the validation set. Because of this,
the validation error tends to under-predict the classification error of
new data.</p>
<p>For this reason, it is recommended to split the data into three sets:</p>
<ul class="simple">
<li>The <strong>training set</strong>, used to train the model (usually ~60% of the
data)</li>
<li>The <strong>validation set</strong>, used to validate the model (usually ~20% of
the data)</li>
<li>The <strong>test set</strong>, used to evaluate the expected error of the
validated model (usually ~20% of the data)</li>
</ul>
<p>Many machine learning practitioners do not separate test set and
validation set. But if your goal is to gauge the error of a model on
unknown data, using an independent test set is vital.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="section" id="examples-for-the-scikit-learn-chapter">
<h2><a class="toc-backref" href="#id34">3.6.10. Examples for the scikit-learn chapter</a><a class="headerlink" href="#examples-for-the-scikit-learn-chapter" title="Permalink to this headline">¶</a></h2>
<div class="sphx-glr-thumbcontainer" tooltip="Demonstrates overfit when testing on train set. "><div class="figure" id="id8">
<img alt="../../_images/sphx_glr_plot_measuring_performance_thumb.png" src="../../_images/sphx_glr_plot_measuring_performance_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_measuring_performance.html#sphx-glr-packages-scikit-learn-auto-examples-plot-measuring-performance-py"><span class="std std-ref">Measuring Decision Tree performance</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Demo PCA in 2D"><div class="figure" id="id9">
<img alt="../../_images/sphx_glr_plot_pca_thumb.png" src="../../_images/sphx_glr_plot_pca_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_pca.html#sphx-glr-packages-scikit-learn-auto-examples-plot-pca-py"><span class="std std-ref">Demo PCA in 2D</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="A simple linear regression"><div class="figure" id="id10">
<img alt="../../_images/sphx_glr_plot_linear_regression_thumb.png" src="../../_images/sphx_glr_plot_linear_regression_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_linear_regression.html#sphx-glr-packages-scikit-learn-auto-examples-plot-linear-regression-py"><span class="std std-ref">A simple linear regression</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Plot a simple scatter plot of 2 features of the iris dataset."><div class="figure" id="id11">
<img alt="../../_images/sphx_glr_plot_iris_scatter_thumb.png" src="../../_images/sphx_glr_plot_iris_scatter_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_iris_scatter.html#sphx-glr-packages-scikit-learn-auto-examples-plot-iris-scatter-py"><span class="std std-ref">Plot 2D views of the iris dataset</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Here we use :class:`sklearn.manifold.TSNE` to visualize the digits datasets. Indeed, the digits..."><div class="figure" id="id12">
<img alt="../../_images/sphx_glr_plot_tsne_thumb.png" src="../../_images/sphx_glr_plot_tsne_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_tsne.html#sphx-glr-packages-scikit-learn-auto-examples-plot-tsne-py"><span class="std std-ref">tSNE to visualize digits</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip=" "><div class="figure" id="id13">
<img alt="../../_images/sphx_glr_plot_linear_model_cv_thumb.png" src="../../_images/sphx_glr_plot_linear_model_cv_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_linear_model_cv.html#sphx-glr-packages-scikit-learn-auto-examples-plot-linear-model-cv-py"><span class="std std-ref">Use the RidgeCV and LassoCV to set the regularization parameter</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip=" "><div class="figure" id="id14">
<img alt="../../_images/sphx_glr_plot_variance_linear_regr_thumb.png" src="../../_images/sphx_glr_plot_variance_linear_regr_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_variance_linear_regr.html#sphx-glr-packages-scikit-learn-auto-examples-plot-variance-linear-regr-py"><span class="std std-ref">Plot variance and regularization in linear models</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="This example generates simple synthetic data ploints and shows a separating hyperplane on them...."><div class="figure" id="id15">
<img alt="../../_images/sphx_glr_plot_separator_thumb.png" src="../../_images/sphx_glr_plot_separator_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_separator.html#sphx-glr-packages-scikit-learn-auto-examples-plot-separator-py"><span class="std std-ref">Simple picture of the formal problem of machine learning</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Compare the performance of a variety of classifiers on a test set for the digits data. "><div class="figure" id="id16">
<img alt="../../_images/sphx_glr_plot_compare_classifiers_thumb.png" src="../../_images/sphx_glr_plot_compare_classifiers_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_compare_classifiers.html#sphx-glr-packages-scikit-learn-auto-examples-plot-compare-classifiers-py"><span class="std std-ref">Compare classifiers on the digits data</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Fits data generated from a 9th order polynomial with model of 4th order and 9th order polynomia..."><div class="figure" id="id17">
<img alt="../../_images/sphx_glr_plot_polynomial_regression_thumb.png" src="../../_images/sphx_glr_plot_polynomial_regression_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_polynomial_regression.html#sphx-glr-packages-scikit-learn-auto-examples-plot-polynomial-regression-py"><span class="std std-ref">Plot fitting a 9th order polynomial</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Here we perform a simple regression analysis on the California housing data, exploring two type..."><div class="figure" id="id18">
<img alt="../../_images/sphx_glr_plot_california_prediction_thumb.png" src="../../_images/sphx_glr_plot_california_prediction_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_california_prediction.html#sphx-glr-packages-scikit-learn-auto-examples-plot-california-prediction-py"><span class="std std-ref">A simple regression analysis on the California housing data</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Plot the decision boundary of nearest neighbor decision on iris, first with a single nearest ne..."><div class="figure" id="id19">
<img alt="../../_images/sphx_glr_plot_iris_knn_thumb.png" src="../../_images/sphx_glr_plot_iris_knn_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_iris_knn.html#sphx-glr-packages-scikit-learn-auto-examples-plot-iris-knn-py"><span class="std std-ref">Nearest-neighbor prediction on iris</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Plot the first few samples of the digits dataset and a 2D representation built using PCA, then ..."><div class="figure" id="id20">
<img alt="../../_images/sphx_glr_plot_digits_simple_classif_thumb.png" src="../../_images/sphx_glr_plot_digits_simple_classif_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_digits_simple_classif.html#sphx-glr-packages-scikit-learn-auto-examples-plot-digits-simple-classif-py"><span class="std std-ref">Simple visualization and classification of the digits dataset</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="The goal of this example is to show how an unsupervised method and a supervised one can be chai..."><div class="figure" id="id21">
<img alt="../../_images/sphx_glr_plot_eigenfaces_thumb.png" src="../../_images/sphx_glr_plot_eigenfaces_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_eigenfaces.html#sphx-glr-packages-scikit-learn-auto-examples-plot-eigenfaces-py"><span class="std std-ref">The eigenfaces example: chaining PCA and SVMs</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="This is an example plot from the tutorial which accompanies an explanation of the support vecto..."><div class="figure" id="id22">
<img alt="../../_images/sphx_glr_plot_svm_non_linear_thumb.png" src="../../_images/sphx_glr_plot_svm_non_linear_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_svm_non_linear.html#sphx-glr-packages-scikit-learn-auto-examples-plot-svm-non-linear-py"><span class="std std-ref">Example of linear and non-linear models</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="Demo overfitting, underfitting, and validation and learning curves with polynomial regression."><div class="figure" id="id23">
<img alt="../../_images/sphx_glr_plot_bias_variance_thumb.png" src="../../_images/sphx_glr_plot_bias_variance_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_bias_variance.html#sphx-glr-packages-scikit-learn-auto-examples-plot-bias-variance-py"><span class="std std-ref">Bias and variance of polynomial fit</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer" tooltip="This script plots the flow-charts used in the scikit-learn tutorials. "><div class="figure" id="id24">
<img alt="../../_images/sphx_glr_plot_ML_flow_chart_thumb.png" src="../../_images/sphx_glr_plot_ML_flow_chart_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="auto_examples/plot_ML_flow_chart.html#sphx-glr-packages-scikit-learn-auto-examples-plot-ml-flow-chart-py"><span class="std std-ref">Tutorial Diagrams</span></a></span></p>
</div>
</div><div class="toctree-wrapper compound">
</div>
<div style='clear:both'></div><div class="sphx-glr-footer class sphx-glr-footer-gallery docutils container">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../_downloads/auto_examples_python9.zip" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">all</span> <span class="pre">examples</span> <span class="pre">in</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">auto_examples_python.zip</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../_downloads/auto_examples_jupyter9.zip" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">all</span> <span class="pre">examples</span> <span class="pre">in</span> <span class="pre">Jupyter</span> <span class="pre">notebooks:</span> <span class="pre">auto_examples_jupyter.zip</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p><strong>Going further</strong></p>
<ul class="last simple">
<li>The <a class="reference external" href="http://scikit-learn.org">documentation of scikit-learn</a> is
very complete and didactic.</li>
<li><a class="reference external" href="http://shop.oreilly.com/product/0636920030515.do">Introduction to Machine Learning with Python</a>,
by Sarah Guido, Andreas Müller
(<a class="reference external" href="https://github.com/amueller/introduction_to_ml_with_python">notebooks available here</a>).</li>
</ul>
</div>
<p><div style="clear: both"></div></p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">3.6. scikit-learn: machine learning in Python</a><ul>
<li><a class="reference internal" href="#introduction-problem-settings">3.6.1. Introduction: problem settings</a><ul>
<li><a class="reference internal" href="#what-is-machine-learning">3.6.1.1. What is machine learning?</a></li>
<li><a class="reference internal" href="#data-in-scikit-learn">3.6.1.2. Data in scikit-learn</a><ul>
<li><a class="reference internal" href="#the-data-matrix">The data matrix</a></li>
<li><a class="reference internal" href="#a-simple-example-the-iris-dataset">A Simple Example: the Iris Dataset</a><ul>
<li><a class="reference internal" href="#the-application-problem">The application problem</a></li>
<li><a class="reference internal" href="#loading-the-iris-data-with-scikit-learn">Loading the Iris Data with Scikit-learn</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#basic-principles-of-machine-learning-with-scikit-learn">3.6.2. Basic principles of machine learning with scikit-learn</a><ul>
<li><a class="reference internal" href="#introducing-the-scikit-learn-estimator-object">3.6.2.1. Introducing the scikit-learn estimator object</a><ul>
<li><a class="reference internal" href="#fitting-on-data">Fitting on data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#supervised-learning-classification-and-regression">3.6.2.2. Supervised Learning: Classification and regression</a></li>
<li><a class="reference internal" href="#a-recap-on-scikit-learn-s-estimator-interface">3.6.2.3. A recap on Scikit-learn’s estimator interface</a></li>
<li><a class="reference internal" href="#regularization-what-it-is-and-why-it-is-necessary">3.6.2.4. Regularization: what it is and why it is necessary</a><ul>
<li><a class="reference internal" href="#prefering-simpler-models">Prefering simpler models</a></li>
<li><a class="reference internal" href="#simple-versus-complex-models-for-classification">Simple versus complex models for classification</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#supervised-learning-classification-of-handwritten-digits">3.6.3. Supervised Learning: Classification of Handwritten Digits</a><ul>
<li><a class="reference internal" href="#the-nature-of-the-data">3.6.3.1. The nature of the data</a></li>
<li><a class="reference internal" href="#visualizing-the-data-on-its-principal-components">3.6.3.2. Visualizing the Data on its principal components</a></li>
<li><a class="reference internal" href="#gaussian-naive-bayes-classification">3.6.3.3. Gaussian Naive Bayes Classification</a></li>
<li><a class="reference internal" href="#quantitative-measurement-of-performance">3.6.3.4. Quantitative Measurement of Performance</a></li>
</ul>
</li>
<li><a class="reference internal" href="#supervised-learning-regression-of-housing-data">3.6.4. Supervised Learning: Regression of Housing Data</a><ul>
<li><a class="reference internal" href="#a-quick-look-at-the-data">3.6.4.1. A quick look at the data</a></li>
<li><a class="reference internal" href="#predicting-home-prices-a-simple-linear-regression">3.6.4.2. Predicting Home Prices: a Simple Linear Regression</a></li>
</ul>
</li>
<li><a class="reference internal" href="#measuring-prediction-performance">3.6.5. Measuring prediction performance</a><ul>
<li><a class="reference internal" href="#a-quick-test-on-the-k-neighbors-classifier">3.6.5.1. A quick test on the K-neighbors classifier</a></li>
<li><a class="reference internal" href="#a-correct-approach-using-a-validation-set">3.6.5.2. A correct approach: Using a validation set</a></li>
<li><a class="reference internal" href="#model-selection-via-validation">3.6.5.3. Model Selection via Validation</a></li>
<li><a class="reference internal" href="#cross-validation">3.6.5.4. Cross-validation</a></li>
<li><a class="reference internal" href="#hyperparameter-optimization-with-cross-validation">3.6.5.5. Hyperparameter optimization with cross-validation</a><ul>
<li><a class="reference internal" href="#basic-hyperparameter-optimization">Basic Hyperparameter Optimization</a></li>
<li><a class="reference internal" href="#automatically-performing-grid-search">Automatically Performing Grid Search</a></li>
<li><a class="reference internal" href="#built-in-hyperparameter-search">Built-in Hyperparameter Search</a></li>
<li><a class="reference internal" href="#nested-cross-validation">Nested cross-validation</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#unsupervised-learning-dimensionality-reduction-and-visualization">3.6.6. Unsupervised Learning: Dimensionality Reduction and Visualization</a><ul>
<li><a class="reference internal" href="#dimensionality-reduction-pca">3.6.6.1. Dimensionality Reduction: PCA</a></li>
<li><a class="reference internal" href="#visualization-with-a-non-linear-embedding-tsne">3.6.6.2. Visualization with a non-linear embedding: tSNE</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-eigenfaces-example-chaining-pca-and-svms">3.6.7. The eigenfaces example: chaining PCA and SVMs</a></li>
<li><a class="reference internal" href="#id1">3.6.8. The eigenfaces example: chaining PCA and SVMs</a><ul>
<li><a class="reference internal" href="#preprocessing-principal-component-analysis">3.6.8.1. Preprocessing: Principal Component Analysis</a></li>
<li><a class="reference internal" href="#doing-the-learning-support-vector-machines">3.6.8.2. Doing the Learning: Support Vector Machines</a></li>
<li><a class="reference internal" href="#pipelining">3.6.8.3. Pipelining</a></li>
</ul>
</li>
<li><a class="reference internal" href="#parameter-selection-validation-and-testing">3.6.9. Parameter selection, Validation, and Testing</a><ul>
<li><a class="reference internal" href="#hyperparameters-over-fitting-and-under-fitting">3.6.9.1. Hyperparameters, Over-fitting, and Under-fitting</a><ul>
<li><a class="reference internal" href="#bias-variance-trade-off-illustration-on-a-simple-regression-problem">Bias-variance trade-off: illustration on a simple regression problem</a></li>
</ul>
</li>
<li><a class="reference internal" href="#visualizing-the-bias-variance-tradeoff">3.6.9.2. Visualizing the Bias/Variance Tradeoff</a><ul>
<li><a class="reference internal" href="#validation-curves">Validation Curves</a></li>
<li><a class="reference internal" href="#learning-curves">Learning Curves</a></li>
</ul>
</li>
<li><a class="reference internal" href="#summary-on-model-selection">3.6.9.3. Summary on model selection</a><ul>
<li><a class="reference internal" href="#high-bias">High Bias</a></li>
<li><a class="reference internal" href="#high-variance">High Variance</a></li>
</ul>
</li>
<li><a class="reference internal" href="#a-last-word-of-caution-separate-validation-and-test-set">3.6.9.4. A last word of caution: separate validation and test set</a></li>
</ul>
</li>
<li><a class="reference internal" href="#examples-for-the-scikit-learn-chapter">3.6.10. Examples for the scikit-learn chapter</a><ul>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../3d_plotting/index.html"
                        title="previous chapter">3.5. 3D plotting with Mayavi</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="auto_examples/plot_measuring_performance.html"
                        title="next chapter">3.6.10.1. Measuring Decision Tree performance</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/packages/scikit-learn/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="auto_examples/plot_measuring_performance.html" title="3.6.10.1. Measuring Decision Tree performance"
             >next</a></li>
        <li class="right" >
          <a href="../3d_plotting/index.html" title="3.5. 3D plotting with Mayavi"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">Scipy lecture notes</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >3. Packages and applications</a> &#187;</li>
     
    <!-- Insert a menu in the navigation bar -->
    <li class="left">
	<!-- On click toggle the 'tip' on or off-->
	<a onclick="$('.tip').each(function (index, obj) {
			    collapse_tip_div(obj);
		    });
		    $('.tip').addClass('collapsed');
		    $('.left').addClass('transparent');">
	<img src="../../_static/minus.png"
         alt="Collapse to compact view" style="padding: 1ex;"/>
	<span class="hiddenlink">Collapse document to compact view</span>
    </a></li>
    <li class="right edit_on_github"><a href="https://github.com/scipy-lectures/scipy-lecture-notes/edit/master/packages/scikit-learn/index.rst">Edit
    <span class="tooltip">
	Improve this page:<br/>Edit it on Github.
    </span>
    </a>
    </li>

      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2012,2013,2015,2016,2017,2018,2019,2020,2021,2022.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.9.
    </div>
  </body>
</html>