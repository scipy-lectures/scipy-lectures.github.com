
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>3.6.10.14. The eigenfaces example: chaining PCA and SVMs &#8212; Scipy lecture notes</title>
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/gallery.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/copybutton.js"></script>
    <link rel="author" title="About these documents" href="../../../about.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="3.6.10.15. Example of linear and non-linear models" href="plot_svm_non_linear.html" />
    <link rel="prev" title="3.6.10.13. Simple visualization and classification of the digits dataset" href="plot_digits_simple_classif.html" />
   
    <link rel="stylesheet"
	  href="https://unpkg.com/purecss@1.0.0/build/base-min.css">

<script type="text/javascript">
$(function () {
    // Highlight the table of content as we scroll
    sections = {},
    i        = 0,
    url	 = document.URL.replace(/#.*$/, ""),
    current_section = 0;

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50;
    });

    $(window).scroll(function(event) {
	var pos   = $(window).scrollTop();

	// Highlight the current section
	$('a.internal').parent().removeClass('active');
        for(i in sections){
            if(sections[i] > pos){
		break;
            };
	    if($('a.internal[href$="' + i + '"]').is(':visible')){
		current_section = i;
	    };
        }
	$('a.internal[href$="' + current_section + '"]').parent().addClass('active');
	$('a.internal[href$="' + current_section + '"]').parent().parent().parent().addClass('active');
	$('a.internal[href$="' + current_section + '"]').parent().parent().parent().parent().parent().addClass('active');
    });

});
</script>


  </head><body>
   <!-- Use the header to add javascript -->
    

    <script type="text/javascript">
    // Function to collapse the tip divs
    function collapse_tip_div(obj){
	// Update the representation on the tip div based on whether it
	// has the 'collapsed' css class or not: we only want to
	// collapse divs that are not already collapsed
	if($(obj).hasClass("collapsed")) {
	} else {
	    $(obj).find("p.summary").remove();
	    var content = $(obj).text();
	    var html = $(obj).html();

	    if(content.length > 40) {
		if ($.browser.msie) {
		    // We start at '3' to avoid 'tip', as IE
		    // does not count whitespace
		    var content = content.substr(3, 50);
		} else {
		    // We start at '5' to avoid 'tip '
		    var content = content.substr(5, 50);
		}
	    }
	    $(obj).html('<p class="summary"><img src="../../../_static/plus.png">' + content + '...</p>' + html);
	}
    }
    </script>

    <script type="text/javascript">
    $(function () {
	$(".tip")
	    .click(function(event){
		$(this).toggleClass("collapsed");
		// Change state of the global button
		$('div.related li.transparent').removeClass('transparent')
		$(this).find("p.summary").remove();
		if($(this).hasClass("collapsed")) {
		    var content = $(this).text();
		    var html = $(this).html();

		    if(content.length > 40) {
			if ($.browser.msie) {
			    // We start at '3' to avoid 'tip', as IE
			    // does not count whitespace
			    var content = content.substr(3, 50);
			} else {
			    // We start at '5' to avoid 'tip '
			    var content = content.substr(5, 50);
			}
		    }
		    $(this).html('<p class="summary"><img src="../../../_static/plus.png">' + content + '...</p>' + html);
		}
		if (event.target.tagName.toLowerCase() != "a") {
                   return true; //Makes links clickable
		}
	});
    });
    </script>


    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="plot_svm_non_linear.html" title="3.6.10.15. Example of linear and non-linear models"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="plot_digits_simple_classif.html" title="3.6.10.13. Simple visualization and classification of the digits dataset"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">Scipy lecture notes</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >3. Packages and applications</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../index.html" accesskey="U">3.6. scikit-learn: machine learning in Python</a> &#187;</li>
     
    <!-- Insert a menu in the navigation bar -->
    <li class="left">
	<!-- On click toggle the 'tip' on or off-->
	<a onclick="$('.tip').each(function (index, obj) {
			    collapse_tip_div(obj);
		    });
		    $('.tip').addClass('collapsed');
		    $('.left').addClass('transparent');">
	<img src="../../../_static/minus.png"
         alt="Collapse to compact view" style="padding: 1ex;"/>
	<span class="hiddenlink">Collapse document to compact view</span>
    </a></li>
    <li class="right edit_on_github"><a href="https://github.com/scipy-lectures/scipy-lecture-notes/edit/master/packages/scikit-learn/auto_examples/plot_eigenfaces.rst">Edit
    <span class="tooltip">
	Improve this page:<br/>Edit it on Github.
    </span>
    </a>
    </li>

      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="#sphx-glr-download-packages-scikit-learn-auto-examples-plot-eigenfaces-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="the-eigenfaces-example-chaining-pca-and-svms">
<span id="sphx-glr-packages-scikit-learn-auto-examples-plot-eigenfaces-py"></span><h1>3.6.10.14. The eigenfaces example: chaining PCA and SVMs<a class="headerlink" href="#the-eigenfaces-example-chaining-pca-and-svms" title="Permalink to this headline">¶</a></h1>
<p>The goal of this example is to show how an unsupervised method and a
supervised one can be chained for better prediction. It starts with a
didactic but lengthy way of doing things, and finishes with the
idiomatic approach to pipelining in scikit-learn.</p>
<p>Here we’ll take a look at a simple facial recognition example. Ideally,
we would use a dataset consisting of a subset of the <a class="reference external" href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in
the Wild</a> data that is available
with <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html#sklearn.datasets.fetch_lfw_people" title="(in scikit-learn v1.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.datasets.fetch_lfw_people()</span></code></a>. However, this is a
relatively large download (~200MB) so we will do the tutorial on a
simpler, less rich dataset. Feel free to explore the LFW dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<div class="newline"></div><span class="n">faces</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces" title="View documentation for sklearn.datasets.fetch_olivetti_faces"><span class="n">datasets</span><span class="o">.</span><span class="n">fetch_olivetti_faces</span></a><span class="p">()</span>
<div class="newline"></div><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
<div class="newline"></div></pre></div>
</div>
<p>Let’s visualize these faces to see what we’re working with</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<div class="newline"></div><span class="n">fig</span> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure" title="View documentation for matplotlib.pyplot.figure"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span></a><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<div class="newline"></div><span class="c1"># plot several images</span>
<div class="newline"></div><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">15</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../../_images/sphx_glr_plot_eigenfaces_001.png" class="sphx-glr-single-img" src="../../../_images/sphx_glr_plot_eigenfaces_001.png" />
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Note is that these faces have already been localized and scaled to a
common size. This is an important preprocessing piece for facial
recognition, and is a process that can require a large collection of
training data. This can be done in scikit-learn, but the challenge is
gathering a sufficient amount of training data for the algorithm to work.
Fortunately, this piece is common enough that it has been done. One good
resource is
<a class="reference external" href="https://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html">OpenCV</a>,
the <em>Open Computer Vision Library</em>.</p>
</div>
<p>We’ll perform a Support Vector classification of the images. We’ll do a
typical train-test split on the images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split" title="View documentation for sklearn.model_selection.train_test_split"><span class="n">train_test_split</span></a>
<div class="newline"></div><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split" title="View documentation for sklearn.model_selection.train_test_split"><span class="n">train_test_split</span></a><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
<div class="newline"></div>        <span class="n">faces</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(300, 4096) (100, 4096)
<div class="newline"></div></pre></div>
</div>
<div class="section" id="preprocessing-principal-component-analysis">
<h2>Preprocessing: Principal Component Analysis<a class="headerlink" href="#preprocessing-principal-component-analysis" title="Permalink to this headline">¶</a></h2>
<p>1850 dimensions is a lot for SVM. We can use PCA to reduce these 1850
features to a manageable size, while maintaining most of the information
in the dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<div class="newline"></div><span class="n">pca</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="View documentation for sklearn.decomposition.PCA"><span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span></a><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<div class="newline"></div><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p>One interesting part of PCA is that it computes the “mean” face, which
can be interesting to examine:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html#matplotlib.pyplot.imshow" title="View documentation for matplotlib.pyplot.imshow"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span></a><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
<div class="newline"></div>           <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../../_images/sphx_glr_plot_eigenfaces_002.png" class="sphx-glr-single-img" src="../../../_images/sphx_glr_plot_eigenfaces_002.png" />
<p>The principal components measure deviations about this mean along
orthogonal axes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(150, 4096)
<div class="newline"></div></pre></div>
</div>
<p>It is also interesting to visualize these principal components:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure" title="View documentation for matplotlib.pyplot.figure"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span></a><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<div class="newline"></div><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
<div class="newline"></div>              <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../../_images/sphx_glr_plot_eigenfaces_003.png" class="sphx-glr-single-img" src="../../../_images/sphx_glr_plot_eigenfaces_003.png" />
<p>The components (“eigenfaces”) are ordered by their importance from
top-left to bottom-right. We see that the first few components seem to
primarily take care of lighting conditions; the remaining components
pull out certain identifying features: the nose, eyes, eyebrows, etc.</p>
<p>With this projection computed, we can now project our original training
and test data onto the PCA basis:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<div class="newline"></div><span class="n">X_test_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><span class="n">X_train_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(300, 150)
<div class="newline"></div></pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_test_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(100, 150)
<div class="newline"></div></pre></div>
</div>
<p>These projected components correspond to factors in a linear combination
of component images such that the combination approaches the original
face.</p>
</div>
<div class="section" id="doing-the-learning-support-vector-machines">
<h2>Doing the Learning: Support Vector Machines<a class="headerlink" href="#doing-the-learning-support-vector-machines" title="Permalink to this headline">¶</a></h2>
<p>Now we’ll perform support-vector-machine classification on this reduced
dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<div class="newline"></div><span class="n">clf</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="View documentation for sklearn.svm.SVC"><span class="n">svm</span><span class="o">.</span><span class="n">SVC</span></a><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<div class="newline"></div><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<p>Finally, we can evaluate how well this classification did. First, we
might plot a few of the test-cases with the labels learned from the
training set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<div class="newline"></div><span class="n">fig</span> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure" title="View documentation for matplotlib.pyplot.figure"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span></a><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<div class="newline"></div><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">15</span><span class="p">):</span>
<div class="newline"></div>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
<div class="newline"></div>              <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis" title="View documentation for numpy.newaxis"><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span></a><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<div class="newline"></div>    <span class="n">color</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;black&#39;</span> <span class="k">if</span> <span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span>
<div class="newline"></div>    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;small&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<div class="newline"></div></pre></div>
</div>
<img alt="../../../_images/sphx_glr_plot_eigenfaces_004.png" class="sphx-glr-single-img" src="../../../_images/sphx_glr_plot_eigenfaces_004.png" />
<p>The classifier is correct on an impressive number of images given the
simplicity of its learning model! Using a linear classifier on 150
features derived from the pixel-level data, the algorithm correctly
identifies a large number of the people in the images.</p>
<p>Again, we can quantify this effectiveness using one of several measures
from <a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics" title="(in scikit-learn v1.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a>. First we can do the classification
report, which shows the precision, recall and other measures of the
“goodness” of the classification:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<div class="newline"></div><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" title="View documentation for sklearn.metrics.classification_report"><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span></a><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>precision    recall  f1-score   support
<div class="newline"></div>
<div class="newline"></div>           0       1.00      0.50      0.67         6
<div class="newline"></div>           1       1.00      1.00      1.00         4
<div class="newline"></div>           2       0.50      1.00      0.67         2
<div class="newline"></div>           3       1.00      1.00      1.00         1
<div class="newline"></div>           4       0.33      1.00      0.50         1
<div class="newline"></div>           5       1.00      1.00      1.00         5
<div class="newline"></div>           6       1.00      1.00      1.00         4
<div class="newline"></div>           7       1.00      0.67      0.80         3
<div class="newline"></div>           9       1.00      1.00      1.00         1
<div class="newline"></div>          10       1.00      1.00      1.00         4
<div class="newline"></div>          11       1.00      1.00      1.00         1
<div class="newline"></div>          12       0.67      1.00      0.80         2
<div class="newline"></div>          13       1.00      1.00      1.00         3
<div class="newline"></div>          14       1.00      1.00      1.00         5
<div class="newline"></div>          15       1.00      1.00      1.00         3
<div class="newline"></div>          17       1.00      1.00      1.00         6
<div class="newline"></div>          19       1.00      1.00      1.00         4
<div class="newline"></div>          20       1.00      1.00      1.00         1
<div class="newline"></div>          21       1.00      1.00      1.00         1
<div class="newline"></div>          22       1.00      1.00      1.00         2
<div class="newline"></div>          23       1.00      1.00      1.00         1
<div class="newline"></div>          24       1.00      1.00      1.00         2
<div class="newline"></div>          25       1.00      0.50      0.67         2
<div class="newline"></div>          26       1.00      0.75      0.86         4
<div class="newline"></div>          27       1.00      1.00      1.00         1
<div class="newline"></div>          28       0.67      1.00      0.80         2
<div class="newline"></div>          29       1.00      1.00      1.00         3
<div class="newline"></div>          30       1.00      1.00      1.00         4
<div class="newline"></div>          31       1.00      1.00      1.00         3
<div class="newline"></div>          32       1.00      1.00      1.00         3
<div class="newline"></div>          33       1.00      1.00      1.00         2
<div class="newline"></div>          34       1.00      1.00      1.00         3
<div class="newline"></div>          35       1.00      1.00      1.00         1
<div class="newline"></div>          36       1.00      1.00      1.00         3
<div class="newline"></div>          37       1.00      1.00      1.00         3
<div class="newline"></div>          38       1.00      1.00      1.00         1
<div class="newline"></div>          39       1.00      1.00      1.00         3
<div class="newline"></div>
<div class="newline"></div>    accuracy                           0.94       100
<div class="newline"></div>   macro avg       0.95      0.96      0.94       100
<div class="newline"></div>weighted avg       0.97      0.94      0.94       100
<div class="newline"></div></pre></div>
</div>
<p>Another interesting metric is the <em>confusion matrix</em>, which indicates
how often any two items are mixed-up. The confusion matrix of a perfect
classifier would only have nonzero entries on the diagonal, with zeros
on the off-diagonal:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="View documentation for sklearn.metrics.confusion_matrix"><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span></a><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[3 0 0 ... 0 0 0]
<div class="newline"></div> [0 4 0 ... 0 0 0]
<div class="newline"></div> [0 0 2 ... 0 0 0]
<div class="newline"></div> ...
<div class="newline"></div> [0 0 0 ... 3 0 0]
<div class="newline"></div> [0 0 0 ... 0 1 0]
<div class="newline"></div> [0 0 0 ... 0 0 3]]
<div class="newline"></div></pre></div>
</div>
</div>
<div class="section" id="pipelining">
<h2>Pipelining<a class="headerlink" href="#pipelining" title="Permalink to this headline">¶</a></h2>
<p>Above we used PCA as a pre-processing step before applying our support
vector machine classifier. Plugging the output of one estimator directly
into the input of a second estimator is a commonly used pattern; for
this reason scikit-learn provides a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> object which automates
this process. The above problem can be re-expressed as a pipeline as
follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="View documentation for sklearn.pipeline.Pipeline"><span class="n">Pipeline</span></a>
<div class="newline"></div><span class="n">clf</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="View documentation for sklearn.pipeline.Pipeline"><span class="n">Pipeline</span></a><span class="p">([(</span><span class="s1">&#39;pca&#39;</span><span class="p">,</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="View documentation for sklearn.decomposition.PCA"><span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span></a><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">)),</span>
<div class="newline"></div>                <span class="p">(</span><span class="s1">&#39;svm&#39;</span><span class="p">,</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="View documentation for sklearn.svm.LinearSVC"><span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span></a><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))])</span>
<div class="newline"></div>
<div class="newline"></div><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<div class="newline"></div><span class="nb">print</span><span class="p">(</span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="View documentation for sklearn.metrics.confusion_matrix"><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span></a><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<div class="newline"></div><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="View documentation for matplotlib.pyplot.show"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
<div class="newline"></div></pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[5 0 0 ... 0 0 0]
<div class="newline"></div> [0 4 0 ... 0 0 0]
<div class="newline"></div> [0 0 1 ... 0 0 0]
<div class="newline"></div> ...
<div class="newline"></div> [0 0 0 ... 3 0 0]
<div class="newline"></div> [0 0 0 ... 0 1 0]
<div class="newline"></div> [0 0 0 ... 0 0 3]]
<div class="newline"></div></pre></div>
</div>
</div>
<div class="section" id="a-note-on-facial-recognition">
<h2>A Note on Facial Recognition<a class="headerlink" href="#a-note-on-facial-recognition" title="Permalink to this headline">¶</a></h2>
<p>Here we have used PCA “eigenfaces” as a pre-processing step for facial
recognition. The reason we chose this is because PCA is a
broadly-applicable technique, which can be useful for a wide array of
data types. Research in the field of facial recognition in particular,
however, has shown that other more specific feature extraction methods
are can be much more effective.</p>
<p><strong>Total running time of the script:</strong> ( 0 minutes  2.276 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-packages-scikit-learn-auto-examples-plot-eigenfaces-py">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../../_downloads/plot_eigenfaces.py" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_eigenfaces.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../../_downloads/plot_eigenfaces.ipynb" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_eigenfaces.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
<p><div style="clear: both"></div></p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">3.6.10.14. The eigenfaces example: chaining PCA and SVMs</a><ul>
<li><a class="reference internal" href="#preprocessing-principal-component-analysis">Preprocessing: Principal Component Analysis</a></li>
<li><a class="reference internal" href="#doing-the-learning-support-vector-machines">Doing the Learning: Support Vector Machines</a></li>
<li><a class="reference internal" href="#pipelining">Pipelining</a></li>
<li><a class="reference internal" href="#a-note-on-facial-recognition">A Note on Facial Recognition</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="plot_digits_simple_classif.html"
                        title="previous chapter">3.6.10.13. Simple visualization and classification of the digits dataset</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="plot_svm_non_linear.html"
                        title="next chapter">3.6.10.15. Example of linear and non-linear models</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../../_sources/packages/scikit-learn/auto_examples/plot_eigenfaces.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="plot_svm_non_linear.html" title="3.6.10.15. Example of linear and non-linear models"
             >next</a></li>
        <li class="right" >
          <a href="plot_digits_simple_classif.html" title="3.6.10.13. Simple visualization and classification of the digits dataset"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">Scipy lecture notes</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >3. Packages and applications</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../index.html" >3.6. scikit-learn: machine learning in Python</a> &#187;</li>
     
    <!-- Insert a menu in the navigation bar -->
    <li class="left">
	<!-- On click toggle the 'tip' on or off-->
	<a onclick="$('.tip').each(function (index, obj) {
			    collapse_tip_div(obj);
		    });
		    $('.tip').addClass('collapsed');
		    $('.left').addClass('transparent');">
	<img src="../../../_static/minus.png"
         alt="Collapse to compact view" style="padding: 1ex;"/>
	<span class="hiddenlink">Collapse document to compact view</span>
    </a></li>
    <li class="right edit_on_github"><a href="https://github.com/scipy-lectures/scipy-lecture-notes/edit/master/packages/scikit-learn/auto_examples/plot_eigenfaces.rst">Edit
    <span class="tooltip">
	Improve this page:<br/>Edit it on Github.
    </span>
    </a>
    </li>

      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2012,2013,2015,2016,2017,2018,2019,2020,2021,2022.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.9.
    </div>
  </body>
</html>