.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_advanced_mathematical_optimization_auto_examples_plot_1d_optim.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_mathematical_optimization_auto_examples_plot_1d_optim.py:


Brent's method
================

Illustration of 1D optimization: Brent's method




.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /advanced/mathematical_optimization/auto_examples/images/sphx_glr_plot_1d_optim_001.png
            :class: sphx-glr-multi-img

    *

      .. image:: /advanced/mathematical_optimization/auto_examples/images/sphx_glr_plot_1d_optim_002.png
            :class: sphx-glr-multi-img

    *

      .. image:: /advanced/mathematical_optimization/auto_examples/images/sphx_glr_plot_1d_optim_003.png
            :class: sphx-glr-multi-img

    *

      .. image:: /advanced/mathematical_optimization/auto_examples/images/sphx_glr_plot_1d_optim_004.png
            :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Converged at  6
    Converged at  23




|


.. code-block:: python


    import numpy as np
    import matplotlib.pyplot as plt
    from scipy import optimize

    x = np.linspace(-1, 3, 100)
    x_0 = np.exp(-1)

    def f(x):
        return (x - x_0)**2 + epsilon*np.exp(-5*(x - .5 - x_0)**2)

    for epsilon in (0, 1):
        plt.figure(figsize=(3, 2.5))
        plt.axes([0, 0, 1, 1])

        # A convex function
        plt.plot(x, f(x), linewidth=2)

        # Apply brent method. To have access to the iteration, do this in an
        # artificial way: allow the algorithm to iter only once
        all_x = list()
        all_y = list()
        for iter in range(30):
            result = optimize.minimize_scalar(f, bracket=(-5, 2.9, 4.5), method="Brent",
                        options={"maxiter": iter}, tol=np.finfo(1.).eps)
            if result.success:
                print('Converged at ', iter)
                break

            this_x = result.x
            all_x.append(this_x)
            all_y.append(f(this_x))
            if iter < 6:
                plt.text(this_x - .05*np.sign(this_x) - .05,
                        f(this_x) + 1.2*(.3 - iter % 2), iter + 1,
                        size=12)

        plt.plot(all_x[:10], all_y[:10], 'k+', markersize=12, markeredgewidth=2)

        plt.plot(all_x[-1], all_y[-1], 'rx', markersize=12)
        plt.axis('off')
        plt.ylim(ymin=-1, ymax=8)

        plt.figure(figsize=(4, 3))
        plt.semilogy(np.abs(all_y - all_y[-1]), linewidth=2)
        plt.ylabel('Error on f(x)')
        plt.xlabel('Iteration')
        plt.tight_layout()

    plt.show()


**Total running time of the script:** ( 0 minutes  0.279 seconds)


.. _sphx_glr_download_advanced_mathematical_optimization_auto_examples_plot_1d_optim.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_1d_optim.py <plot_1d_optim.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_1d_optim.ipynb <plot_1d_optim.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
